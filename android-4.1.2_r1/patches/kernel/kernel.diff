diff --git a/arch/arm/configs/tegra3_android_eventlogging_defconfig b/arch/arm/configs/tegra3_android_eventlogging_defconfig
new file mode 100644
index 0000000..6d2144a
--- /dev/null
+++ b/arch/arm/configs/tegra3_android_eventlogging_defconfig
@@ -0,0 +1,555 @@
+
+CONFIG_EXPERIMENTAL=y
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_CGROUPS=y
+CONFIG_CGROUP_DEBUG=y
+CONFIG_CGROUP_FREEZER=y
+CONFIG_CGROUP_CPUACCT=y
+CONFIG_RESOURCE_COUNTERS=y
+CONFIG_CGROUP_SCHED=y
+CONFIG_RT_GROUP_SCHED=y
+CONFIG_BLK_DEV_INITRD=y
+CONFIG_PANIC_TIMEOUT=10
+# CONFIG_SYSCTL_SYSCALL is not set
+# CONFIG_ELF_CORE is not set
+CONFIG_ASHMEM=y
+CONFIG_EMBEDDED=y
+CONFIG_SLAB=y
+CONFIG_PROFILING=y
+CONFIG_OPROFILE=y
+CONFIG_MODULES=y
+CONFIG_MODULE_UNLOAD=y
+CONFIG_MODULE_FORCE_UNLOAD=y
+# CONFIG_BLK_DEV_BSG is not set
+CONFIG_ARCH_TEGRA=y
+CONFIG_GPIO_PCA953X=y
+CONFIG_ARCH_TEGRA_3x_SOC=y
+CONFIG_TEGRA_PCI=y
+CONFIG_MACH_CARDHU=y
+CONFIG_MACH_TEGRA_ENTERPRISE=y
+CONFIG_MACH_GROUPER=y
+CONFIG_TEGRA_PWM=y
+CONFIG_TEGRA_FIQ_DEBUGGER=y
+CONFIG_TEGRA_EMC_SCALING_ENABLE=y
+CONFIG_WIFI_CONTROL_FUNC=y
+CONFIG_TEGRA_CLOCK_DEBUG_WRITE=y
+CONFIG_USB_HOTPLUG=y
+CONFIG_TEGRA_DYNAMIC_PWRDET=y
+# CONFIG_TEGRA_THERMAL_SYSFS is not set
+CONFIG_TEGRA_PLLM_RESTRICTED=y
+CONFIG_ARM_ERRATA_742230=y
+CONFIG_ARM_ERRATA_743622=y
+CONFIG_ARM_ERRATA_751472=y
+CONFIG_ARM_ERRATA_752520=y
+CONFIG_FIQ_DEBUGGER_CONSOLE=y
+CONFIG_NO_HZ=y
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_SMP=y
+CONFIG_PREEMPT=y
+CONFIG_AEABI=y
+# CONFIG_OABI_COMPAT is not set
+CONFIG_HIGHMEM=y
+CONFIG_ARM_FLUSH_CONSOLE_ON_RESTART=y
+CONFIG_ZBOOT_ROM_TEXT=0x0
+CONFIG_ZBOOT_ROM_BSS=0x0
+CONFIG_CMDLINE="tegra_wdt.heartbeat=30"
+CONFIG_CMDLINE_EXTEND=y
+CONFIG_CPU_FREQ=y
+CONFIG_CPU_FREQ_DEFAULT_GOV_USERSPACE=y
+CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
+CONFIG_CPU_FREQ_GOV_POWERSAVE=y
+CONFIG_CPU_FREQ_GOV_ONDEMAND=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
+CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_IDLE=y
+CONFIG_VFP=y
+CONFIG_NEON=y
+CONFIG_WAKELOCK=y
+CONFIG_PM_RUNTIME=y
+CONFIG_PM_DEBUG=y
+CONFIG_SUSPEND_TIME=y
+CONFIG_NET=y
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_NET_KEY=y
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+CONFIG_IP_ADVANCED_ROUTER=y
+CONFIG_IP_MULTIPLE_TABLES=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+CONFIG_IP_PNP_RARP=y
+CONFIG_INET_ESP=y
+# CONFIG_INET_XFRM_MODE_BEET is not set
+# CONFIG_INET_LRO is not set
+# CONFIG_INET_DIAG is not set
+CONFIG_IPV6=y
+CONFIG_IPV6_PRIVACY=y
+CONFIG_IPV6_ROUTER_PREF=y
+CONFIG_IPV6_OPTIMISTIC_DAD=y
+CONFIG_INET6_AH=y
+CONFIG_INET6_ESP=y
+CONFIG_INET6_IPCOMP=y
+CONFIG_IPV6_MIP6=y
+CONFIG_IPV6_TUNNEL=y
+CONFIG_IPV6_MULTIPLE_TABLES=y
+CONFIG_NETFILTER=y
+CONFIG_NF_CONNTRACK=y
+CONFIG_NF_CONNTRACK_EVENTS=y
+CONFIG_NF_CT_PROTO_DCCP=y
+CONFIG_NF_CT_PROTO_SCTP=y
+CONFIG_NF_CT_PROTO_UDPLITE=y
+CONFIG_NF_CONNTRACK_AMANDA=y
+CONFIG_NF_CONNTRACK_FTP=y
+CONFIG_NF_CONNTRACK_H323=y
+CONFIG_NF_CONNTRACK_IRC=y
+CONFIG_NF_CONNTRACK_NETBIOS_NS=y
+CONFIG_NF_CONNTRACK_PPTP=y
+CONFIG_NF_CONNTRACK_SANE=y
+CONFIG_NF_CONNTRACK_TFTP=y
+CONFIG_NF_CT_NETLINK=y
+CONFIG_NETFILTER_TPROXY=y
+CONFIG_NETFILTER_XT_TARGET_CLASSIFY=y
+CONFIG_NETFILTER_XT_TARGET_CONNMARK=y
+CONFIG_NETFILTER_XT_TARGET_IDLETIMER=y
+CONFIG_NETFILTER_XT_TARGET_MARK=y
+CONFIG_NETFILTER_XT_TARGET_NFLOG=y
+CONFIG_NETFILTER_XT_TARGET_NFQUEUE=y
+CONFIG_NETFILTER_XT_TARGET_TPROXY=y
+CONFIG_NETFILTER_XT_TARGET_TRACE=y
+CONFIG_NETFILTER_XT_MATCH_COMMENT=y
+CONFIG_NETFILTER_XT_MATCH_CONNBYTES=y
+CONFIG_NETFILTER_XT_MATCH_CONNLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_CONNMARK=y
+CONFIG_NETFILTER_XT_MATCH_CONNTRACK=y
+CONFIG_NETFILTER_XT_MATCH_HASHLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_HELPER=y
+CONFIG_NETFILTER_XT_MATCH_IPRANGE=y
+CONFIG_NETFILTER_XT_MATCH_LENGTH=y
+CONFIG_NETFILTER_XT_MATCH_LIMIT=y
+CONFIG_NETFILTER_XT_MATCH_MAC=y
+CONFIG_NETFILTER_XT_MATCH_MARK=y
+CONFIG_NETFILTER_XT_MATCH_POLICY=y
+CONFIG_NETFILTER_XT_MATCH_PKTTYPE=y
+CONFIG_NETFILTER_XT_MATCH_QTAGUID=y
+CONFIG_NETFILTER_XT_MATCH_QUOTA2=y
+CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG=y
+CONFIG_NETFILTER_XT_MATCH_SOCKET=y
+CONFIG_NETFILTER_XT_MATCH_STATE=y
+CONFIG_NETFILTER_XT_MATCH_STATISTIC=y
+CONFIG_NETFILTER_XT_MATCH_STRING=y
+CONFIG_NETFILTER_XT_MATCH_TIME=y
+CONFIG_NETFILTER_XT_MATCH_U32=y
+CONFIG_NF_CONNTRACK_IPV4=y
+CONFIG_IP_NF_IPTABLES=y
+CONFIG_IP_NF_MATCH_AH=y
+CONFIG_IP_NF_MATCH_ECN=y
+CONFIG_IP_NF_MATCH_TTL=y
+CONFIG_IP_NF_FILTER=y
+CONFIG_IP_NF_TARGET_REJECT=y
+CONFIG_IP_NF_TARGET_REJECT_SKERR=y
+CONFIG_IP_NF_TARGET_LOG=y
+CONFIG_NF_NAT=y
+CONFIG_IP_NF_TARGET_MASQUERADE=y
+CONFIG_IP_NF_TARGET_NETMAP=y
+CONFIG_IP_NF_TARGET_REDIRECT=y
+CONFIG_IP_NF_MANGLE=y
+CONFIG_IP_NF_RAW=y
+CONFIG_IP_NF_ARPTABLES=y
+CONFIG_IP_NF_ARPFILTER=y
+CONFIG_IP_NF_ARP_MANGLE=y
+CONFIG_NF_CONNTRACK_IPV6=y
+CONFIG_IP6_NF_IPTABLES=y
+CONFIG_IP6_NF_TARGET_LOG=y
+CONFIG_IP6_NF_FILTER=y
+CONFIG_IP6_NF_TARGET_REJECT=y
+CONFIG_IP6_NF_TARGET_REJECT_SKERR=y
+CONFIG_IP6_NF_MANGLE=y
+CONFIG_IP6_NF_RAW=y
+CONFIG_NET_SCHED=y
+CONFIG_NET_SCH_HTB=y
+CONFIG_NET_SCH_INGRESS=y
+CONFIG_NET_CLS_U32=y
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_U32=y
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=y
+CONFIG_NET_ACT_GACT=y
+CONFIG_NET_ACT_MIRRED=y
+CONFIG_BT=y
+CONFIG_BT_L2CAP=y
+CONFIG_BT_SCO=y
+CONFIG_BT_RFCOMM=y
+CONFIG_BT_RFCOMM_TTY=y
+CONFIG_BT_BNEP=y
+CONFIG_BT_HIDP=y
+CONFIG_BT_HCIUART=y
+CONFIG_BT_HCIUART_H4=y
+CONFIG_BT_HCIUART_LL=y
+CONFIG_BT_BLUESLEEP=y
+CONFIG_CFG80211=y
+CONFIG_NL80211_TESTMODE=y
+CONFIG_RFKILL=y
+CONFIG_CAIF=y
+CONFIG_NFC=y
+CONFIG_PN544_NFC=y
+# CONFIG_FIRMWARE_IN_KERNEL is not set
+CONFIG_BLK_DEV_LOOP=y
+CONFIG_MISC_DEVICES=y
+CONFIG_AD525X_DPOT=y
+CONFIG_AD525X_DPOT_I2C=y
+CONFIG_APDS9802ALS=y
+CONFIG_SENSORS_NCT1008=y
+CONFIG_UID_STAT=y
+CONFIG_BCM4330_RFKILL=y
+CONFIG_TEGRA_CRYPTO_DEV=y
+CONFIG_MAX1749_VIBRATOR=y
+CONFIG_EEPROM_AT24=y
+# CONFIG_MPU_SENSORS_TIMERIRQ is not set
+# CONFIG_MPU_SENSORS_MPU6050B1 is not set
+# CONFIG_MPU_SENSORS_AMI306 is not set
+
+#
+# Magnetometer sensors
+#
+# CONFIG_SENSORS_HMC5843 is not set
+CONFIG_AMI306=y
+
+#
+# Light sensors
+#
+CONFIG_SENSORS_ISL29028=y
+CONFIG_SENSORS_LTR558=y
+
+#
+# Active energy metering IC
+#
+CONFIG_IIO=y
+CONFIG_IIO_BUFFER=y
+CONFIG_IIO_KFIFO_BUF=y
+CONFIG_IIO_TRIGGER=y
+CONFIG_IIO_CONSUMERS_PER_TRIGGER=2
+CONFIG_INV_MPU_IIO=y
+
+CONFIG_TEGRA_BB_SUPPORT=y
+CONFIG_TEGRA_BB_POWER=y
+CONFIG_TEGRA_BB_M7400=y
+CONFIG_SCSI=y
+CONFIG_BLK_DEV_SD=y
+CONFIG_BLK_DEV_SR=y
+CONFIG_BLK_DEV_SR_VENDOR=y
+CONFIG_CHR_DEV_SG=y
+CONFIG_SCSI_MULTI_LUN=y
+CONFIG_MD=y
+CONFIG_BLK_DEV_DM=y
+CONFIG_DM_CRYPT=y
+CONFIG_DM_UEVENT=y
+CONFIG_NETDEVICES=y
+CONFIG_DUMMY=y
+CONFIG_TUN=y
+# CONFIG_NETDEV_10000 is not set
+CONFIG_BCMDHD=y
+CONFIG_BCMDHD_FW_PATH="/system/vendor/firmware/fw_bcmdhd.bin"
+CONFIG_BCMDHD_NVRAM_PATH="/system/etc/nvram.txt"
+CONFIG_DHD_ENABLE_P2P=y
+CONFIG_USB_USBNET=y
+CONFIG_USB_NET_SMSC95XX=y
+# CONFIG_USB_NET_NET1080 is not set
+# CONFIG_USB_BELKIN is not set
+# CONFIG_USB_ARMLINUX is not set
+# CONFIG_USB_NET_ZAURUS is not set
+CONFIG_PPP=y
+CONFIG_PPP_FILTER=y
+CONFIG_PPP_ASYNC=y
+CONFIG_PPP_SYNC_TTY=y
+CONFIG_PPP_DEFLATE=y
+CONFIG_PPP_BSDCOMP=y
+CONFIG_PPP_MPPE=y
+CONFIG_PPPOLAC=y
+CONFIG_PPPOPNS=y
+# CONFIG_INPUT_MOUSEDEV is not set
+CONFIG_INPUT_EVDEV=y
+CONFIG_INPUT_KEYRESET=y
+CONFIG_INPUT_LID=y
+# CONFIG_KEYBOARD_ATKBD is not set
+CONFIG_KEYBOARD_GPIO=y
+CONFIG_KEYBOARD_TEGRA=y
+# CONFIG_INPUT_MOUSE is not set
+CONFIG_INPUT_JOYSTICK=y
+CONFIG_JOYSTICK_XPAD=y
+CONFIG_JOYSTICK_XPAD_FF=y
+CONFIG_JOYSTICK_XPAD_LEDS=y
+CONFIG_INPUT_TABLET=y
+CONFIG_TABLET_USB_ACECAD=y
+CONFIG_TABLET_USB_AIPTEK=y
+CONFIG_TABLET_USB_GTCO=y
+CONFIG_TABLET_USB_HANWANG=y
+CONFIG_TABLET_USB_KBTAB=y
+CONFIG_TABLET_USB_WACOM=y
+CONFIG_INPUT_TOUCHSCREEN=y
+CONFIG_TOUCHSCREEN_ELAN_TF_3K=y
+CONFIG_TOUCHSCREEN_RM31080A=y
+CONFIG_TOUCHSCREEN_SYN_RMI4_SPI=y
+CONFIG_INPUT_MISC=y
+CONFIG_INPUT_KEYCHORD=y
+CONFIG_INPUT_UINPUT=y
+CONFIG_INPUT_GPIO=y
+CONFIG_SERIO_LIBPS2=y
+# CONFIG_VT is not set
+# CONFIG_LEGACY_PTYS is not set
+# CONFIG_DEVKMEM is not set
+CONFIG_SERIAL_8250=y
+CONFIG_SERIAL_8250_CONSOLE=y
+CONFIG_SERIAL_TEGRA=y
+# CONFIG_HW_RANDOM is not set
+# CONFIG_I2C_COMPAT is not set
+CONFIG_I2C_CHARDEV=y
+CONFIG_I2C_MUX=y
+CONFIG_I2C_MUX_PCA954x=y
+# CONFIG_I2C_HELPER_AUTO is not set
+CONFIG_I2C_TEGRA=y
+CONFIG_SPI=y
+CONFIG_SPI_TEGRA=y
+CONFIG_SPI_SLAVE_TEGRA=y
+CONFIG_DEBUG_GPIO=y
+CONFIG_GPIO_SYSFS=y
+CONFIG_POWER_SUPPLY=y
+CONFIG_CHARGER_SMB347=y
+CONFIG_BATTERY_BQ27541=y
+CONFIG_SENSORS_TEGRA_TSENSOR=y
+CONFIG_SENSORS_INA219=y
+CONFIG_SENSORS_AL3010=y
+CONFIG_THERMAL=y
+CONFIG_WATCHDOG=y
+CONFIG_TEGRA_WATCHDOG=y
+CONFIG_TEGRA_WATCHDOG_ENABLE_ON_PROBE=y
+CONFIG_MFD_TPS6586X=y
+CONFIG_MFD_TPS65910=y
+CONFIG_MFD_MAX77663=y
+CONFIG_MFD_TPS6591X=y
+CONFIG_MFD_TPS80031=y
+CONFIG_GPADC_TPS80031=y
+CONFIG_MFD_RICOH583=y
+CONFIG_REGULATOR=y
+CONFIG_REGULATOR_FIXED_VOLTAGE=y
+CONFIG_REGULATOR_VIRTUAL_CONSUMER=y
+CONFIG_REGULATOR_MAX77663=y
+CONFIG_REGULATOR_TPS6586X=y
+CONFIG_REGULATOR_TPS65910=y
+CONFIG_REGULATOR_TPS62360=y
+CONFIG_REGULATOR_TPS6591X=y
+CONFIG_REGULATOR_TPS80031=y
+CONFIG_REGULATOR_RICOH583=y
+CONFIG_MEDIA_SUPPORT=y
+CONFIG_VIDEO_DEV=y
+# CONFIG_RC_CORE is not set
+# CONFIG_MEDIA_TUNER_CUSTOMISE is not set
+CONFIG_VIDEO_HELPER_CHIPS_AUTO=y
+# CONFIG_TEGRA_AVP is not set
+# CONFIG_TEGRA_MEDIASERVER is not set
+CONFIG_TEGRA_NVAVP=y
+CONFIG_VIDEO_MI1040=y
+CONFIG_USB_VIDEO_CLASS=y
+# CONFIG_USB_GSPCA is not set
+# CONFIG_RADIO_ADAPTERS is not set
+CONFIG_VIDEO_OUTPUT_CONTROL=y
+CONFIG_FB=y
+CONFIG_TEGRA_GRHOST=y
+CONFIG_TEGRA_DC=y
+CONFIG_TEGRA_DSI=y
+CONFIG_TEGRA_NVHDCP=y
+CONFIG_BACKLIGHT_LCD_SUPPORT=y
+CONFIG_LCD_CLASS_DEVICE=y
+CONFIG_BACKLIGHT_CLASS_DEVICE=y
+# CONFIG_BACKLIGHT_GENERIC is not set
+CONFIG_BACKLIGHT_PWM=y
+CONFIG_BACKLIGHT_TEGRA_PWM=y
+CONFIG_SOUND=y
+CONFIG_SND=y
+CONFIG_SND_HDA_INTEL=y
+CONFIG_SND_HDA_PLATFORM_DRIVER=y
+CONFIG_SND_HDA_PLATFORM_NVIDIA_TEGRA=y
+CONFIG_SND_HDA_POWER_SAVE=y
+CONFIG_SND_HDA_POWER_SAVE_DEFAULT=10
+CONFIG_SND_SOC=y
+CONFIG_SND_SOC_TEGRA=y
+CONFIG_SND_SOC_TEGRA_RT5640=y
+CONFIG_HEADSET_FUNCTION=y
+CONFIG_HID_A4TECH=y
+CONFIG_HID_ACRUX=y
+CONFIG_HID_ACRUX_FF=y
+CONFIG_HID_APPLE=y
+CONFIG_HID_BELKIN=y
+CONFIG_HID_CHERRY=y
+CONFIG_HID_CHICONY=y
+CONFIG_HID_CYPRESS=y
+CONFIG_HID_DRAGONRISE=y
+CONFIG_DRAGONRISE_FF=y
+CONFIG_HID_EMS_FF=y
+CONFIG_HID_ELECOM=y
+CONFIG_HID_EZKEY=y
+CONFIG_HID_HOLTEK=y
+CONFIG_HOLTEK_FF=y
+CONFIG_HID_KEYTOUCH=y
+CONFIG_HID_KYE=y
+CONFIG_HID_UCLOGIC=y
+CONFIG_HID_WALTOP=y
+CONFIG_HID_GYRATION=y
+CONFIG_HID_TWINHAN=y
+CONFIG_HID_KENSINGTON=y
+CONFIG_HID_LCPOWER=y
+CONFIG_HID_LOGITECH=y
+CONFIG_LOGITECH_FF=y
+CONFIG_LOGIRUMBLEPAD2_FF=y
+CONFIG_LOGIG940_FF=y
+CONFIG_LOGIWII_FF=y
+CONFIG_HID_MAGICMOUSE=y
+CONFIG_HID_MICROSOFT=y
+CONFIG_HID_MONTEREY=y
+CONFIG_HID_MULTITOUCH=y
+CONFIG_HID_NTRIG=y
+CONFIG_HID_ORTEK=y
+CONFIG_HID_PANTHERLORD=y
+CONFIG_PANTHERLORD_FF=y
+CONFIG_HID_PETALYNX=y
+CONFIG_HID_SONY=y
+CONFIG_HID_SPEEDLINK=y
+CONFIG_HID_SUNPLUS=y
+CONFIG_HID_GREENASIA=y
+CONFIG_GREENASIA_FF=y
+CONFIG_HID_SMARTJOYPLUS=y
+CONFIG_SMARTJOYPLUS_FF=y
+CONFIG_HID_TOPSEED=y
+CONFIG_HID_THRUSTMASTER=y
+CONFIG_THRUSTMASTER_FF=y
+CONFIG_HID_WACOM=y
+CONFIG_HID_WIIMOTE=y
+CONFIG_HID_ZEROPLUS=y
+CONFIG_ZEROPLUS_FF=y
+CONFIG_HID_ZYDACRON=y
+CONFIG_USB_ANNOUNCE_NEW_DEVICES=y
+CONFIG_USB_DEVICEFS=y
+CONFIG_USB_SUSPEND=y
+CONFIG_USB_OTG=y
+# CONFIG_USB_OTG_WHITELIST is not set
+CONFIG_USB_EHCI_HCD=y
+CONFIG_USB_ACM=y
+CONFIG_USB_WDM=y
+CONFIG_USB_STORAGE=y
+CONFIG_USB_LIBUSUAL=y
+CONFIG_USB_SERIAL=y
+CONFIG_USB_SERIAL_PL2303=y
+CONFIG_USB_SERIAL_OPTION=y
+CONFIG_USB_GADGET=y
+CONFIG_USB_GADGET_VBUS_DRAW=500
+CONFIG_USB_FSL_USB2=y
+CONFIG_USB_G_ANDROID=y
+CONFIG_USB_TEGRA_OTG=y
+CONFIG_MMC=y
+CONFIG_MMC_UNSAFE_RESUME=y
+CONFIG_MMC_EMBEDDED_SDIO=y
+CONFIG_MMC_BLOCK_MINORS=16
+CONFIG_MMC_BLOCK_DEFERRED_RESUME=y
+CONFIG_MMC_TEST=y
+CONFIG_MMC_SDHCI=y
+CONFIG_MMC_SDHCI_PLTFM=y
+CONFIG_MMC_SDHCI_TEGRA=y
+CONFIG_NEW_LEDS=y
+CONFIG_LEDS_CLASS=y
+CONFIG_LEDS_GPIO=y
+CONFIG_SWITCH=y
+CONFIG_RTC_CLASS=y
+CONFIG_RTC_DRV_MAX77663=y
+CONFIG_RTC_DRV_TPS6586X=y
+CONFIG_RTC_DRV_TPS6591x=y
+CONFIG_RTC_DRV_TPS80031=y
+CONFIG_RTC_DRV_RC5T583=y
+CONFIG_STAGING=y
+CONFIG_ANDROID=y
+CONFIG_ANDROID_BINDER_IPC=y
+CONFIG_ANDROID_LOGGER=y
+CONFIG_ANDROID_RAM_CONSOLE=y
+CONFIG_ANDROID_RAM_CONSOLE_ERROR_CORRECTION=y
+CONFIG_ANDROID_TIMED_GPIO=y
+CONFIG_ANDROID_LOW_MEMORY_KILLER=y
+CONFIG_EXT2_FS=y
+CONFIG_EXT2_FS_XATTR=y
+CONFIG_EXT2_FS_POSIX_ACL=y
+CONFIG_EXT2_FS_SECURITY=y
+CONFIG_EXT3_FS=y
+# CONFIG_EXT3_DEFAULTS_TO_ORDERED is not set
+CONFIG_EXT3_FS_POSIX_ACL=y
+CONFIG_EXT3_FS_SECURITY=y
+CONFIG_EXT4_FS=y
+CONFIG_EXT4_FS_POSIX_ACL=y
+# CONFIG_DNOTIFY is not set
+CONFIG_FUSE_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_NTFS_FS=y
+CONFIG_TMPFS=y
+CONFIG_NFS_FS=y
+CONFIG_ROOT_NFS=y
+CONFIG_PARTITION_ADVANCED=y
+CONFIG_EFI_PARTITION=y
+CONFIG_NLS_CODEPAGE_437=y
+CONFIG_NLS_ISO8859_1=y
+CONFIG_PRINTK_TIME=y
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_LOCKUP_DETECTOR=y
+# CONFIG_DETECT_HUNG_TASK is not set
+CONFIG_SCHEDSTATS=y
+CONFIG_TIMER_STATS=y
+# CONFIG_DEBUG_PREEMPT is not set
+CONFIG_DEBUG_INFO=y
+CONFIG_DEBUG_VM=y
+# CONFIG_EVENT_POWER_TRACING_DEPRECATED is not set
+CONFIG_ENABLE_DEFAULT_TRACERS=y
+CONFIG_DYNAMIC_DEBUG=y
+CONFIG_TRUSTED_FOUNDATIONS=y
+CONFIG_CRYPTO_SHA256=y
+CONFIG_CRYPTO_TWOFISH=y
+# CONFIG_CRYPTO_ANSI_CPRNG is not set
+CONFIG_CRYPTO_DEV_TEGRA_SE=y
+CONFIG_EVENT_LOGGING=y
+CONFIG_EVENT_CPU_ONLINE=y
+CONFIG_EVENT_CPU_DOWN_PREPARE=y
+CONFIG_EVENT_CPU_DEAD=y
+CONFIG_EVENT_WAKE_LOCK=y
+CONFIG_EVENT_WAKE_UNLOCK=y
+CONFIG_EVENT_CONTEXT_SWITCH=y
+CONFIG_EVENT_PREEMPT_TICK=y
+CONFIG_EVENT_PREEMPT_WAKEUP=y
+CONFIG_EVENT_YIELD=y
+CONFIG_EVENT_WAITQUEUE_WAIT=y
+CONFIG_EVENT_WAITQUEUE_WAKE=y
+CONFIG_EVENT_WAITQUEUE_NOTIFY=y
+CONFIG_EVENT_IDLE_START=n
+CONFIG_EVENT_IDLE_END=n
+CONFIG_EVENT_SUSPEND_START=y
+CONFIG_EVENT_SUSPEND=y
+CONFIG_EVENT_RESUME=y
+CONFIG_EVENT_RESUME_FINISH=y
+CONFIG_EVENT_FORK=y
+CONFIG_EVENT_EXIT=y
+CONFIG_EVENT_IO_BLOCK=y
+CONFIG_EVENT_IO_RESUME=y
+CONFIG_EVENT_DATAGRAM_BLOCK=y
+CONFIG_EVENT_DATAGRAM_RESUME=y
+CONFIG_EVENT_STREAM_BLOCK=y
+CONFIG_EVENT_STREAM_RESUME=y
+CONFIG_EVENT_SOCK_BLOCK=y
+CONFIG_EVENT_SOCK_RESUME=y
+CONFIG_EVENT_SEMAPHORE_LOCK=n
+CONFIG_EVENT_SEMAPHORE_WAIT=y
+CONFIG_EVENT_SEMAPHORE_WAKE=y
+CONFIG_EVENT_SEMAPHORE_NOTIFY=y
+CONFIG_EVENT_MUTEX_LOCK=n
+CONFIG_EVENT_MUTEX_WAIT=y
+CONFIG_EVENT_MUTEX_WAKE=y
+CONFIG_EVENT_MUTEX_NOTIFY=y
+CONFIG_EVENT_FUTEX_WAIT=y
+CONFIG_EVENT_FUTEX_WAKE=y
+CONFIG_EVENT_FUTEX_NOTIFY=y
+CONFIG_EVENT_THREAD_NAME=y
diff --git a/arch/arm/configs/tuna_eventlogging_defconfig b/arch/arm/configs/tuna_eventlogging_defconfig
new file mode 100644
index 0000000..dbc9c70
--- /dev/null
+++ b/arch/arm/configs/tuna_eventlogging_defconfig
@@ -0,0 +1,489 @@
+CONFIG_EXPERIMENTAL=y
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_CGROUPS=y
+CONFIG_CGROUP_DEBUG=y
+CONFIG_CGROUP_FREEZER=y
+CONFIG_CGROUP_CPUACCT=y
+CONFIG_RESOURCE_COUNTERS=y
+CONFIG_CGROUP_SCHED=y
+CONFIG_RT_GROUP_SCHED=y
+CONFIG_BLK_DEV_INITRD=y
+CONFIG_PANIC_TIMEOUT=5
+CONFIG_KALLSYMS_ALL=y
+CONFIG_ASHMEM=y
+# CONFIG_AIO is not set
+CONFIG_EMBEDDED=y
+CONFIG_PERF_COUNTERS=y
+CONFIG_PROFILING=y
+CONFIG_MODULES=y
+CONFIG_MODULE_FORCE_LOAD=y
+CONFIG_MODULE_UNLOAD=y
+CONFIG_MODULE_FORCE_UNLOAD=y
+# CONFIG_BLK_DEV_BSG is not set
+CONFIG_ARCH_OMAP=y
+CONFIG_OMAP_SMARTREFLEX=y
+CONFIG_OMAP_SMARTREFLEX_CLASS3=y
+CONFIG_OMAP_SMARTREFLEX_CLASS1P5=y
+CONFIG_OMAP_RESET_CLOCKS=y
+CONFIG_OMAP_DM_TIMER_DEBUG=y
+CONFIG_OMAP_TEMP_SENSOR=y
+CONFIG_OMAP_REMOTEPROC_MEMPOOL_SIZE=0x0
+# CONFIG_ARCH_OMAP2 is not set
+# CONFIG_ARCH_OMAP3 is not set
+# CONFIG_MACH_OMAP_4430SDP is not set
+CONFIG_OMAP_ALLOW_OSWR=y
+CONFIG_ARM_THUMBEE=y
+CONFIG_PL310_ERRATA_727915=y
+CONFIG_FIQ_DEBUGGER_CONSOLE=y
+CONFIG_ARM_ERRATA_764369=y
+CONFIG_NO_HZ=y
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_SMP=y
+CONFIG_NR_CPUS=2
+CONFIG_PREEMPT=y
+CONFIG_HIGHMEM=y
+CONFIG_COMPACTION=y
+CONFIG_ARM_FLUSH_CONSOLE_ON_RESTART=y
+CONFIG_CMDLINE="console=ttyFIQ0 androidboot.console=ttyFIQ0 mem=1G vmalloc=768M omap_wdt.timer_margin=30 no_console_suspend"
+CONFIG_CMDLINE_EXTEND=y
+CONFIG_CPU_FREQ=y
+CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE=y
+CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
+CONFIG_CPU_FREQ_GOV_POWERSAVE=y
+CONFIG_CPU_FREQ_GOV_USERSPACE=y
+CONFIG_CPU_FREQ_GOV_ONDEMAND=y
+CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_HOTPLUG=y
+CONFIG_CPU_IDLE=y
+# CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+CONFIG_BINFMT_MISC=y
+CONFIG_WAKELOCK=y
+CONFIG_PM_DEBUG=y
+CONFIG_SUSPEND_TIME=y
+CONFIG_NET=y
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM_USER=y
+CONFIG_NET_KEY=y
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+CONFIG_IP_ADVANCED_ROUTER=y
+CONFIG_IP_MULTIPLE_TABLES=y
+CONFIG_INET_ESP=y
+# CONFIG_INET_XFRM_MODE_BEET is not set
+# CONFIG_INET_LRO is not set
+CONFIG_IPV6=y
+CONFIG_IPV6_PRIVACY=y
+CONFIG_IPV6_ROUTER_PREF=y
+CONFIG_IPV6_OPTIMISTIC_DAD=y
+CONFIG_INET6_AH=y
+CONFIG_INET6_ESP=y
+CONFIG_INET6_IPCOMP=y
+CONFIG_IPV6_MIP6=y
+CONFIG_IPV6_TUNNEL=y
+CONFIG_IPV6_MULTIPLE_TABLES=y
+CONFIG_NETFILTER=y
+# CONFIG_BRIDGE_NETFILTER is not set
+CONFIG_NF_CONNTRACK=y
+CONFIG_NF_CONNTRACK_EVENTS=y
+CONFIG_NF_CT_PROTO_DCCP=y
+CONFIG_NF_CT_PROTO_SCTP=y
+CONFIG_NF_CT_PROTO_UDPLITE=y
+CONFIG_NF_CONNTRACK_AMANDA=y
+CONFIG_NF_CONNTRACK_FTP=y
+CONFIG_NF_CONNTRACK_H323=y
+CONFIG_NF_CONNTRACK_IRC=y
+CONFIG_NF_CONNTRACK_NETBIOS_NS=y
+CONFIG_NF_CONNTRACK_PPTP=y
+CONFIG_NF_CONNTRACK_SANE=y
+CONFIG_NF_CONNTRACK_TFTP=y
+CONFIG_NF_CT_NETLINK=y
+CONFIG_NETFILTER_TPROXY=y
+CONFIG_NETFILTER_XT_TARGET_CLASSIFY=y
+CONFIG_NETFILTER_XT_TARGET_CONNMARK=y
+CONFIG_NETFILTER_XT_TARGET_IDLETIMER=y
+CONFIG_NETFILTER_XT_TARGET_MARK=y
+CONFIG_NETFILTER_XT_TARGET_NFLOG=y
+CONFIG_NETFILTER_XT_TARGET_NFQUEUE=y
+CONFIG_NETFILTER_XT_TARGET_TPROXY=y
+CONFIG_NETFILTER_XT_TARGET_TRACE=y
+CONFIG_NETFILTER_XT_MATCH_COMMENT=y
+CONFIG_NETFILTER_XT_MATCH_CONNBYTES=y
+CONFIG_NETFILTER_XT_MATCH_CONNLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_CONNMARK=y
+CONFIG_NETFILTER_XT_MATCH_CONNTRACK=y
+CONFIG_NETFILTER_XT_MATCH_HASHLIMIT=y
+CONFIG_NETFILTER_XT_MATCH_HELPER=y
+CONFIG_NETFILTER_XT_MATCH_IPRANGE=y
+CONFIG_NETFILTER_XT_MATCH_LENGTH=y
+CONFIG_NETFILTER_XT_MATCH_LIMIT=y
+CONFIG_NETFILTER_XT_MATCH_MAC=y
+CONFIG_NETFILTER_XT_MATCH_MARK=y
+CONFIG_NETFILTER_XT_MATCH_POLICY=y
+CONFIG_NETFILTER_XT_MATCH_PKTTYPE=y
+CONFIG_NETFILTER_XT_MATCH_QTAGUID=y
+CONFIG_NETFILTER_XT_MATCH_QUOTA=y
+CONFIG_NETFILTER_XT_MATCH_QUOTA2=y
+CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG=y
+CONFIG_NETFILTER_XT_MATCH_SOCKET=y
+CONFIG_NETFILTER_XT_MATCH_STATE=y
+CONFIG_NETFILTER_XT_MATCH_STATISTIC=y
+CONFIG_NETFILTER_XT_MATCH_STRING=y
+CONFIG_NETFILTER_XT_MATCH_TIME=y
+CONFIG_NETFILTER_XT_MATCH_U32=y
+CONFIG_NF_CONNTRACK_IPV4=y
+CONFIG_IP_NF_IPTABLES=y
+CONFIG_IP_NF_MATCH_AH=y
+CONFIG_IP_NF_MATCH_ECN=y
+CONFIG_IP_NF_MATCH_TTL=y
+CONFIG_IP_NF_FILTER=y
+CONFIG_IP_NF_TARGET_REJECT=y
+CONFIG_IP_NF_TARGET_REJECT_SKERR=y
+CONFIG_IP_NF_TARGET_LOG=y
+CONFIG_NF_NAT=y
+CONFIG_IP_NF_TARGET_MASQUERADE=y
+CONFIG_IP_NF_TARGET_NETMAP=y
+CONFIG_IP_NF_TARGET_REDIRECT=y
+CONFIG_IP_NF_MANGLE=y
+CONFIG_IP_NF_RAW=y
+CONFIG_IP_NF_ARPTABLES=y
+CONFIG_IP_NF_ARPFILTER=y
+CONFIG_IP_NF_ARP_MANGLE=y
+CONFIG_NF_CONNTRACK_IPV6=y
+CONFIG_IP6_NF_IPTABLES=y
+CONFIG_IP6_NF_TARGET_LOG=y
+CONFIG_IP6_NF_FILTER=y
+CONFIG_IP6_NF_TARGET_REJECT=y
+CONFIG_IP6_NF_TARGET_REJECT_SKERR=y
+CONFIG_IP6_NF_MANGLE=y
+CONFIG_IP6_NF_RAW=y
+CONFIG_BRIDGE=y
+# CONFIG_BRIDGE_IGMP_SNOOPING is not set
+CONFIG_PHONET=y
+CONFIG_NET_SCHED=y
+CONFIG_NET_SCH_HTB=y
+CONFIG_NET_SCH_INGRESS=y
+CONFIG_NET_CLS_U32=y
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_U32=y
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=y
+CONFIG_NET_ACT_GACT=y
+CONFIG_NET_ACT_MIRRED=y
+CONFIG_BT=y
+CONFIG_BT_L2CAP=y
+CONFIG_BT_SCO=y
+CONFIG_BT_RFCOMM=y
+CONFIG_BT_RFCOMM_TTY=y
+CONFIG_BT_BNEP=y
+CONFIG_BT_HIDP=y
+CONFIG_BT_HCIUART=y
+CONFIG_BT_HCIUART_H4=y
+CONFIG_CFG80211=y
+CONFIG_NL80211_TESTMODE=y
+# CONFIG_CFG80211_WEXT is not set
+CONFIG_CFG80211_ALLOW_RECONNECT=y
+CONFIG_RFKILL=y
+CONFIG_RFKILL_INPUT=y
+CONFIG_MTD=y
+CONFIG_MTD_CHAR=y
+CONFIG_MTD_BLOCK=y
+CONFIG_MTD_M25P80=y
+CONFIG_MTD_NAND_IDS=y
+CONFIG_MTD_ONENAND=y
+CONFIG_BLK_DEV_LOOP=y
+CONFIG_BLK_DEV_RAM=y
+CONFIG_BLK_DEV_RAM_SIZE=8192
+CONFIG_MISC_DEVICES=y
+CONFIG_SAMSUNG_JACK=y
+CONFIG_UID_STAT=y
+CONFIG_BMP180=y
+CONFIG_USB_SWITCH_FSA9480=y
+CONFIG_OMAP_DIE_TEMP_SENSOR=y
+CONFIG_LEDS_AN30259A=y
+CONFIG_MPU_SENSORS_TIMERIRQ=y
+CONFIG_INV_SENSORS=y
+CONFIG_MPU_SENSORS_MPU3050=y
+CONFIG_MPU_SENSORS_BMA250=y
+CONFIG_MPU_SENSORS_YAS530=y
+CONFIG_SEC_MODEM=y
+CONFIG_UMTS_LINK_MIPI=y
+CONFIG_UMTS_MODEM_XMM6260=y
+CONFIG_CDMA_LINK_DPRAM=y
+CONFIG_CDMA_MODEM_CBP71=y
+CONFIG_LTE_LINK_USB=y
+CONFIG_LTE_MODEM_CMC221=y
+CONFIG_SCSI=y
+CONFIG_BLK_DEV_SD=y
+CONFIG_CHR_DEV_SG=y
+CONFIG_MD=y
+CONFIG_BLK_DEV_DM=y
+CONFIG_DM_DEBUG=y
+CONFIG_DM_CRYPT=y
+CONFIG_DM_UEVENT=y
+CONFIG_NETDEVICES=y
+CONFIG_IFB=y
+CONFIG_TUN=y
+CONFIG_WIFI_CONTROL_FUNC=y
+CONFIG_BCMDHD=y
+CONFIG_BCMDHD_FW_PATH="/system/vendor/firmware/fw_bcmdhd.bin"
+CONFIG_DHD_ENABLE_P2P=y
+CONFIG_USB_USBNET=y
+CONFIG_PPP=y
+CONFIG_PPP_DEFLATE=y
+CONFIG_PPP_BSDCOMP=y
+CONFIG_PPP_MPPE=y
+CONFIG_PPPOLAC=y
+CONFIG_PPPOPNS=y
+# CONFIG_INPUT_MOUSEDEV is not set
+CONFIG_INPUT_EVDEV=y
+CONFIG_INPUT_KEYRESET=y
+# CONFIG_KEYBOARD_ATKBD is not set
+CONFIG_KEYBOARD_OMAP4=y
+# CONFIG_INPUT_MOUSE is not set
+CONFIG_INPUT_JOYSTICK=y
+CONFIG_JOYSTICK_XPAD=y
+CONFIG_JOYSTICK_XPAD_FF=y
+CONFIG_INPUT_TABLET=y
+CONFIG_TABLET_USB_ACECAD=y
+CONFIG_TABLET_USB_AIPTEK=y
+CONFIG_TABLET_USB_GTCO=y
+CONFIG_TABLET_USB_HANWANG=y
+CONFIG_TABLET_USB_KBTAB=y
+CONFIG_TABLET_USB_WACOM=y
+CONFIG_INPUT_TOUCHSCREEN=y
+CONFIG_TOUCHSCREEN_ATMEL_MXT=y
+CONFIG_TOUCHSCREEN_MMS=y
+CONFIG_INPUT_MISC=y
+CONFIG_INPUT_KEYCHORD=y
+CONFIG_INPUT_UINPUT=y
+CONFIG_INPUT_GPIO=y
+CONFIG_OPTICAL_GP2A=y
+CONFIG_SERIO_LIBPS2=y
+# CONFIG_VT is not set
+# CONFIG_LEGACY_PTYS is not set
+CONFIG_HW_RANDOM=y
+CONFIG_I2C_CHARDEV=y
+CONFIG_I2C_GPIO=y
+CONFIG_SPI=y
+CONFIG_SPI_OMAP24XX=y
+CONFIG_GPIO_SYSFS=y
+CONFIG_GPIO_TWL4030=y
+CONFIG_POWER_SUPPLY=y
+CONFIG_PDA_POWER=y
+CONFIG_BATTERY_MAX17040=y
+# CONFIG_HWMON is not set
+CONFIG_WATCHDOG=y
+CONFIG_OMAP_WATCHDOG=y
+CONFIG_TWL6030_POWER=y
+CONFIG_TWL6030_PWM=y
+CONFIG_TWL6030_MADC=y
+CONFIG_REGULATOR_TWL4030=y
+CONFIG_MEDIA_SUPPORT=y
+# CONFIG_RC_CORE is not set
+CONFIG_PVR_SGX=y
+CONFIG_PVR_NEED_PVR_DPF=y
+CONFIG_PVR_NEED_PVR_ASSERT=y
+CONFIG_PVR_USSE_EDM_STATUS_DEBUG=y
+CONFIG_SGX_DVFS_MODE_OPTIMIZED=y
+CONFIG_PVR_LINUX_MEM_AREA_POOL=y
+CONFIG_ION=y
+CONFIG_ION_OMAP=y
+CONFIG_FB=y
+CONFIG_SII9234=y
+CONFIG_FB_OMAP_BOOTLOADER_INIT=y
+CONFIG_OMAP2_DSS=y
+CONFIG_OMAP2_VRAM_SIZE=16
+# CONFIG_OMAP2_DSS_DPI is not set
+# CONFIG_OMAP2_DSS_VENC is not set
+CONFIG_OMAP2_DSS_DSI=y
+CONFIG_FB_OMAP2=y
+CONFIG_PANEL_S6E8AA0=y
+CONFIG_OMAP4_HDCP=y
+CONFIG_BACKLIGHT_LCD_SUPPORT=y
+# CONFIG_LCD_CLASS_DEVICE is not set
+CONFIG_DISPLAY_SUPPORT=y
+CONFIG_SOUND=y
+CONFIG_SND=y
+CONFIG_SND_SOC=y
+CONFIG_SND_OMAP_SOC=y
+CONFIG_SND_OMAP_SOC_SDP4430=y
+CONFIG_SND_OMAP_SOC_OMAP4_HDMI=y
+CONFIG_HID_A4TECH=y
+CONFIG_HID_ACRUX=y
+CONFIG_HID_ACRUX_FF=y
+CONFIG_HID_APPLE=y
+CONFIG_HID_BELKIN=y
+CONFIG_HID_CHERRY=y
+CONFIG_HID_CHICONY=y
+CONFIG_HID_CYPRESS=y
+CONFIG_HID_DRAGONRISE=y
+CONFIG_DRAGONRISE_FF=y
+CONFIG_HID_EMS_FF=y
+CONFIG_HID_ELECOM=y
+CONFIG_HID_EZKEY=y
+CONFIG_HID_KEYTOUCH=y
+CONFIG_HID_KYE=y
+CONFIG_HID_UCLOGIC=y
+CONFIG_HID_WALTOP=y
+CONFIG_HID_GYRATION=y
+CONFIG_HID_TWINHAN=y
+CONFIG_HID_KENSINGTON=y
+CONFIG_HID_LCPOWER=y
+CONFIG_HID_LOGITECH=y
+CONFIG_LOGITECH_FF=y
+CONFIG_LOGIRUMBLEPAD2_FF=y
+CONFIG_LOGIG940_FF=y
+CONFIG_LOGIWII_FF=y
+CONFIG_HID_MAGICMOUSE=y
+CONFIG_HID_MICROSOFT=y
+CONFIG_HID_MONTEREY=y
+CONFIG_HID_MULTITOUCH=y
+CONFIG_HID_NTRIG=y
+CONFIG_HID_ORTEK=y
+CONFIG_HID_PANTHERLORD=y
+CONFIG_PANTHERLORD_FF=y
+CONFIG_HID_PETALYNX=y
+CONFIG_HID_PICOLCD=y
+CONFIG_HID_QUANTA=y
+CONFIG_HID_ROCCAT_ARVO=y
+CONFIG_HID_ROCCAT_KONE=y
+CONFIG_HID_ROCCAT_KONEPLUS=y
+CONFIG_HID_ROCCAT_KOVAPLUS=y
+CONFIG_HID_ROCCAT_PYRA=y
+CONFIG_HID_SAMSUNG=y
+CONFIG_HID_SONY=y
+CONFIG_HID_SUNPLUS=y
+CONFIG_HID_GREENASIA=y
+CONFIG_GREENASIA_FF=y
+CONFIG_HID_SMARTJOYPLUS=y
+CONFIG_SMARTJOYPLUS_FF=y
+CONFIG_HID_TOPSEED=y
+CONFIG_HID_THRUSTMASTER=y
+CONFIG_THRUSTMASTER_FF=y
+CONFIG_HID_WACOM=y
+CONFIG_HID_ZEROPLUS=y
+CONFIG_ZEROPLUS_FF=y
+CONFIG_HID_ZYDACRON=y
+CONFIG_USB_ANNOUNCE_NEW_DEVICES=y
+CONFIG_USB_DEVICEFS=y
+CONFIG_USB_SUSPEND=y
+# CONFIG_USB_OTG_WHITELIST is not set
+CONFIG_USB_EHCI_HCD=y
+CONFIG_USB_MUSB_HDRC=y
+CONFIG_USB_MUSB_OMAP2PLUS=y
+CONFIG_USB_MUSB_OTG=y
+CONFIG_USB_GADGET_MUSB_HDRC=y
+CONFIG_USB_ACM=y
+CONFIG_USB_STORAGE=y
+CONFIG_USB_SERIAL=y
+CONFIG_USB_GADGET=y
+CONFIG_USB_GADGET_VBUS_DRAW=500
+CONFIG_USB_G_ANDROID=y
+CONFIG_USB_OTG_WAKELOCK=y
+CONFIG_MMC=y
+CONFIG_MMC_UNSAFE_RESUME=y
+CONFIG_MMC_EMBEDDED_SDIO=y
+CONFIG_MMC_PARANOID_SD_INIT=y
+CONFIG_MMC_OMAP=y
+CONFIG_MMC_OMAP_HS=y
+CONFIG_SWITCH=y
+CONFIG_SWITCH_GPIO=y
+CONFIG_RTC_CLASS=y
+CONFIG_RTC_DRV_TWL4030=y
+CONFIG_STAGING=y
+CONFIG_ANDROID=y
+CONFIG_ANDROID_BINDER_IPC=y
+CONFIG_ANDROID_LOGGER=y
+CONFIG_ANDROID_RAM_CONSOLE=y
+CONFIG_ANDROID_RAM_CONSOLE_ERROR_CORRECTION=y
+CONFIG_ANDROID_TIMED_GPIO=y
+CONFIG_ANDROID_LOW_MEMORY_KILLER=y
+CONFIG_OMAP_HSI=y
+CONFIG_EXT2_FS=y
+CONFIG_EXT4_FS=y
+# CONFIG_EXT4_FS_XATTR is not set
+# CONFIG_DNOTIFY is not set
+CONFIG_FUSE_FS=y
+CONFIG_MSDOS_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_TMPFS=y
+CONFIG_TMPFS_POSIX_ACL=y
+# CONFIG_NETWORK_FILESYSTEMS is not set
+CONFIG_PARTITION_ADVANCED=y
+CONFIG_EFI_PARTITION=y
+CONFIG_NLS_CODEPAGE_437=y
+CONFIG_NLS_ASCII=y
+CONFIG_NLS_ISO8859_1=y
+CONFIG_PRINTK_TIME=y
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_DEBUG_KERNEL=y
+CONFIG_DETECT_HUNG_TASK=y
+CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=10
+# CONFIG_DEBUG_PREEMPT is not set
+CONFIG_DEBUG_SPINLOCK_SLEEP=y
+CONFIG_DEBUG_INFO=y
+CONFIG_SYSCTL_SYSCALL_CHECK=y
+CONFIG_SCHED_TRACER=y
+# CONFIG_ARM_UNWIND is not set
+CONFIG_DEBUG_USER=y
+CONFIG_SECURITY_MIDDLEWARE_COMPONENT=y
+# CONFIG_SMC_KERNEL_CRYPTO is not set
+CONFIG_CRYPTO_SHA256=y
+CONFIG_CRYPTO_TWOFISH=y
+CONFIG_CRC_CCITT=y
+CONFIG_CRYPTO_DEV_TEGRA_SE=y
+CONFIG_EVENT_LOGGING=y
+CONFIG_EVENT_CPU_ONLINE=y
+CONFIG_EVENT_CPU_DOWN_PREPARE=y
+CONFIG_EVENT_CPU_DEAD=y
+CONFIG_EVENT_CPUFREQ_SET=y
+CONFIG_EVENT_BINDER_PRODUCE_ONEWAY=y
+CONFIG_EVENT_BINDER_PRODUCE_TWOWAY=y
+CONFIG_EVENT_BINDER_PRODUCE_REPLY=y
+CONFIG_EVENT_BINDER_CONSUME=y
+CONFIG_EVENT_WAKE_LOCK=y
+CONFIG_EVENT_WAKE_UNLOCK=y
+CONFIG_EVENT_CONTEXT_SWITCH=y
+CONFIG_EVENT_PREEMPT_TICK=y
+CONFIG_EVENT_PREEMPT_WAKEUP=y
+CONFIG_EVENT_YIELD=y
+CONFIG_EVENT_WAITQUEUE_WAIT=y
+CONFIG_EVENT_WAITQUEUE_WAKE=y
+CONFIG_EVENT_WAITQUEUE_NOTIFY=y
+CONFIG_EVENT_IDLE_START=n
+CONFIG_EVENT_IDLE_END=n
+CONFIG_EVENT_SUSPEND_START=y
+CONFIG_EVENT_SUSPEND=y
+CONFIG_EVENT_RESUME=y
+CONFIG_EVENT_RESUME_FINISH=y
+CONFIG_EVENT_FORK=y
+CONFIG_EVENT_EXIT=y
+CONFIG_EVENT_IO_BLOCK=y
+CONFIG_EVENT_IO_RESUME=y
+CONFIG_EVENT_DATAGRAM_BLOCK=y
+CONFIG_EVENT_DATAGRAM_RESUME=y
+CONFIG_EVENT_STREAM_BLOCK=y
+CONFIG_EVENT_STREAM_RESUME=y
+CONFIG_EVENT_SOCK_BLOCK=y
+CONFIG_EVENT_SOCK_RESUME=y
+CONFIG_EVENT_SEMAPHORE_LOCK=n
+CONFIG_EVENT_SEMAPHORE_WAIT=y
+CONFIG_EVENT_SEMAPHORE_WAKE=y
+CONFIG_EVENT_SEMAPHORE_NOTIFY=y
+CONFIG_EVENT_MUTEX_LOCK=n
+CONFIG_EVENT_MUTEX_WAIT=y
+CONFIG_EVENT_MUTEX_WAKE=y
+CONFIG_EVENT_MUTEX_NOTIFY=y
+CONFIG_EVENT_FUTEX_WAIT=y
+CONFIG_EVENT_FUTEX_WAKE=y
+CONFIG_EVENT_FUTEX_NOTIFY=y
+CONFIG_EVENT_THREAD_NAME=y
+CONFIG_EVENT_CPUFREQ_BOOST=y
+CONFIG_EVENT_CPUFREQ_WAKE_UP=y
+CONFIG_EVENT_CPUFREQ_MOD_TIMER=y
+CONFIG_EVENT_CPUFREQ_DEL_TIMER=y
+CONFIG_EVENT_CPUFREQ_TIMER=y
+
+
diff --git a/drivers/cpufreq/cpufreq_interactive.c b/drivers/cpufreq/cpufreq_interactive.c
index 7dbacf0..bd4f50c 100644
--- a/drivers/cpufreq/cpufreq_interactive.c
+++ b/drivers/cpufreq/cpufreq_interactive.c
@@ -34,6 +34,8 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/cpufreq_interactive.h>
 
+#include <eventlogging/events.h>
+
 static atomic_t active_count = ATOMIC_INIT(0);
 
 struct cpufreq_interactive_cpuinfo {
@@ -144,6 +146,8 @@ static void cpufreq_interactive_timer(unsigned long data)
 	if (!pcpu->governor_enabled)
 		goto exit;
 
+	event_log_cpufreq_timer(data);
+
 	/*
 	 * Once pcpu->timer_run_time is updated to >= pcpu->idle_exit_time,
 	 * this lets idle exit know the current idle time sample has
@@ -272,6 +276,7 @@ static void cpufreq_interactive_timer(unsigned long data)
 		spin_lock_irqsave(&up_cpumask_lock, flags);
 		cpumask_set_cpu(data, &up_cpumask);
 		spin_unlock_irqrestore(&up_cpumask_lock, flags);
+		event_log_cpufreq_wake_up();
 		wake_up_process(up_task);
 	}
 
@@ -301,6 +306,7 @@ rearm:
 
 		pcpu->time_in_idle = get_cpu_idle_time_us(
 			data, &pcpu->idle_exit_time);
+		event_log_cpufreq_mod_timer(pcpu->cpu_timer.data, timer_rate);
 		mod_timer(&pcpu->cpu_timer,
 			  jiffies + usecs_to_jiffies(timer_rate));
 	}
@@ -336,6 +342,7 @@ static void cpufreq_interactive_idle_start(void)
 			pcpu->time_in_idle = get_cpu_idle_time_us(
 				smp_processor_id(), &pcpu->idle_exit_time);
 			pcpu->timer_idlecancel = 0;
+			event_log_cpufreq_mod_timer(pcpu->cpu_timer.data, timer_rate);
 			mod_timer(&pcpu->cpu_timer,
 				  jiffies + usecs_to_jiffies(timer_rate));
 		}
@@ -348,6 +355,7 @@ static void cpufreq_interactive_idle_start(void)
 		 * CPU didn't go busy; we'll recheck things upon idle exit.
 		 */
 		if (pending && pcpu->timer_idlecancel) {
+			event_log_cpufreq_del_timer(pcpu->cpu_timer.data);
 			del_timer(&pcpu->cpu_timer);
 			/*
 			 * Ensure last timer run time is after current idle
@@ -387,6 +395,7 @@ static void cpufreq_interactive_idle_end(void)
 			get_cpu_idle_time_us(smp_processor_id(),
 					     &pcpu->idle_exit_time);
 		pcpu->timer_idlecancel = 0;
+		event_log_cpufreq_mod_timer(pcpu->cpu_timer.data, timer_rate);
 		mod_timer(&pcpu->cpu_timer,
 			  jiffies + usecs_to_jiffies(timer_rate));
 	}
@@ -501,6 +510,8 @@ static void cpufreq_interactive_boost(void)
 	unsigned long flags;
 	struct cpufreq_interactive_cpuinfo *pcpu;
 
+	event_log_cpufreq_boost();
+
 	spin_lock_irqsave(&up_cpumask_lock, flags);
 
 	for_each_online_cpu(i) {
@@ -526,8 +537,10 @@ static void cpufreq_interactive_boost(void)
 
 	spin_unlock_irqrestore(&up_cpumask_lock, flags);
 
-	if (anyboost)
+	if (anyboost) {
+		event_log_cpufreq_wake_up();
 		wake_up_process(up_task);
+	}
 }
 
 /*
@@ -878,6 +891,7 @@ static int cpufreq_governor_interactive(struct cpufreq_policy *policy,
 			pcpu = &per_cpu(cpuinfo, j);
 			pcpu->governor_enabled = 0;
 			smp_wmb();
+			event_log_cpufreq_del_timer(pcpu->cpu_timer.data);
 			del_timer_sync(&pcpu->cpu_timer);
 
 			/*
diff --git a/drivers/staging/android/binder.c b/drivers/staging/android/binder.c
index e13b4c4..ef114c6 100644
--- a/drivers/staging/android/binder.c
+++ b/drivers/staging/android/binder.c
@@ -34,6 +34,8 @@
 #include <linux/uaccess.h>
 #include <linux/vmalloc.h>
 
+#include <eventlogging/events.h>
+
 #include "binder.h"
 
 static DEFINE_MUTEX(binder_lock);
@@ -1718,11 +1720,13 @@ static void binder_transaction(struct binder_proc *proc,
 	if (reply) {
 		BUG_ON(t->buffer->async_transaction != 0);
 		binder_pop_transaction(target_thread, in_reply_to);
+		event_log_binder_produce_reply(t);
 	} else if (!(t->flags & TF_ONE_WAY)) {
 		BUG_ON(t->buffer->async_transaction != 0);
 		t->need_reply = 1;
 		t->from_parent = thread->transaction_stack;
 		thread->transaction_stack = t;
+		event_log_binder_produce_twoway(t);
 	} else {
 		BUG_ON(target_node == NULL);
 		BUG_ON(t->buffer->async_transaction != 1);
@@ -1731,6 +1735,7 @@ static void binder_transaction(struct binder_proc *proc,
 			target_wait = NULL;
 		} else
 			target_node->has_async_transaction = 1;
+		event_log_binder_produce_oneway(t);
 	}
 	t->work.type = BINDER_WORK_TRANSACTION;
 	list_add_tail(&t->work.entry, target_list);
@@ -1738,6 +1743,7 @@ static void binder_transaction(struct binder_proc *proc,
 	list_add_tail(&tcomplete->entry, &thread->todo);
 	if (target_wait)
 		wake_up_interruptible(target_wait);
+
 	return;
 
 err_get_unused_fd_failed:
@@ -2458,6 +2464,7 @@ retry:
 			     t->buffer->data_size, t->buffer->offsets_size,
 			     tr.data.ptr.buffer, tr.data.ptr.offsets);
 
+		event_log_binder_consume(t);
 		list_del(&t->work.entry);
 		t->buffer->allow_user_free = 1;
 		if (cmd == BR_TRANSACTION && !(t->flags & TF_ONE_WAY)) {
diff --git a/fs/exec.c b/fs/exec.c
index 044c13f..2b402c8 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -56,6 +56,8 @@
 #include <linux/oom.h>
 #include <linux/compat.h>
 
+#include <eventlogging/events.h>
+
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
 #include <asm/tlb.h>
@@ -1067,6 +1069,7 @@ void set_task_comm(struct task_struct *tsk, char *buf)
 	strlcpy(tsk->comm, buf, sizeof(tsk->comm));
 	task_unlock(tsk);
 	perf_event_comm(tsk);
+	event_log_thread_name(tsk);
 }
 
 int flush_old_exec(struct linux_binprm * bprm)
diff --git a/include/eventlogging/events.h b/include/eventlogging/events.h
new file mode 100644
index 0000000..81bb546
--- /dev/null
+++ b/include/eventlogging/events.h
@@ -0,0 +1,654 @@
+#ifndef EVENTLOGGING_EVENTS_H
+#define EVENTLOGGING_EVENTS_H
+
+#include <linux/types.h>
+
+#define EVENT_LOG_MAGIC "michigan"
+
+#define EVENT_SYNC_LOG 0
+#define EVENT_MISSED_COUNT 1
+
+#define EVENT_CPU_ONLINE 5
+#define EVENT_CPU_DOWN_PREPARE 6
+#define EVENT_CPU_DEAD 7
+#define EVENT_CPUFREQ_SET 8
+
+#define EVENT_PREEMPT_WAKEUP 9
+#define EVENT_CONTEXT_SWITCH 10
+#define EVENT_PREEMPT_TICK 11
+#define EVENT_YIELD 12
+
+#define EVENT_IDLE_START 13
+#define EVENT_IDLE_END 14
+#define EVENT_FORK 15
+#define EVENT_THREAD_NAME 16
+#define EVENT_EXIT 17
+
+#define EVENT_IO_BLOCK 20
+#define EVENT_IO_RESUME 21
+
+#define EVENT_DATAGRAM_BLOCK 30
+#define EVENT_DATAGRAM_RESUME 31
+#define EVENT_STREAM_BLOCK 32
+#define EVENT_STREAM_RESUME 33
+#define EVENT_SOCK_BLOCK 34
+#define EVENT_SOCK_RESUME 35
+
+#define EVENT_SEMAPHORE_LOCK 40
+#define EVENT_SEMAPHORE_WAIT 41
+#define EVENT_SEMAPHORE_WAKE 42
+#define EVENT_SEMAPHORE_NOTIFY 43
+
+#define EVENT_FUTEX_WAIT 46
+#define EVENT_FUTEX_WAKE 47
+#define EVENT_FUTEX_NOTIFY 48
+
+#define EVENT_MUTEX_LOCK 50
+#define EVENT_MUTEX_WAIT 51
+#define EVENT_MUTEX_WAKE 52
+#define EVENT_MUTEX_NOTIFY 53
+
+#define EVENT_WAITQUEUE_WAIT 55
+#define EVENT_WAITQUEUE_WAKE 56
+#define EVENT_WAITQUEUE_NOTIFY 57
+
+#define EVENT_IPC_LOCK 60
+#define EVENT_IPC_WAIT 61
+
+#define EVENT_WAKE_LOCK 70
+#define EVENT_WAKE_UNLOCK 71
+
+#define EVENT_SUSPEND_START 75
+#define EVENT_SUSPEND 76
+#define EVENT_RESUME 77
+#define EVENT_RESUME_FINISH 78
+
+#define EVENT_BINDER_PRODUCE_ONEWAY 90
+#define EVENT_BINDER_PRODUCE_TWOWAY 91
+#define EVENT_BINDER_PRODUCE_REPLY  92
+#define EVENT_BINDER_CONSUME        93
+
+#define EVENT_CPUFREQ_BOOST 100
+#define EVENT_CPUFREQ_WAKE_UP 101
+#define EVENT_CPUFREQ_MOD_TIMER 102
+#define EVENT_CPUFREQ_DEL_TIMER 103
+#define EVENT_CPUFREQ_TIMER 104
+
+#define MAX8 ((1 << 7) - 1)
+#define MIN8 (-(1 << 7))
+
+#define MAX16 ((1 << 15) - 1)
+#define MIN16 (-(1 << 15))
+
+#define MAX24 ((1 << 23) - 1)
+#define MIN24 (-(1 << 23))
+
+struct event_hdr {
+  __u8 event_type;
+  __u8 cpu_tvlen;
+  __le16 pid;
+}__attribute__((packed));
+
+#define CPU_MASK = 0xF0
+#define TVLEN_MASK = 0x0F
+
+#define SET_CPU(header, val) header->cpu_tvlen = (header->cpu_tvlen & ~(0xF0)) | ((val) << 4)
+#define GET_CPU(header) ((header->cpu_tvlen & 0xF0) >> 4)
+
+#define SET_TVLEN(header, sec, usec) do {				\
+    if ((sec) == 4)							\
+      header->cpu_tvlen = (header->cpu_tvlen & ~(0x0F)) | ((usec) << 2); \
+    else								\
+      header->cpu_tvlen = (header->cpu_tvlen & ~(0x0F)) | ((sec) << 2) | (usec); \
+  } while(0)
+
+#define GET_SEC_LEN(header) ({				\
+      int __ret;					\
+      if (0 == (header->cpu_tvlen & 0x03))		\
+	__ret = 4;					\
+      else						\
+	__ret = (header->cpu_tvlen & 0x0C) >> 2;	\
+      __ret;						\
+    })
+
+#define GET_USEC_LEN(header) ({			\
+  int __ret = (header->cpu_tvlen & 0x03);	\
+  if (0 == __ret)				\
+    __ret = (header->cpu_tvlen & 0x0C) >> 2;	\
+  __ret;					\
+})
+
+struct sync_log_event {
+  char magic[8];
+}__attribute__((packed));
+
+struct missed_count_event {
+  __le32 count;
+}__attribute__((packed));
+
+struct context_switch_event {
+  __le16 new_pid;
+  __u8   state;  
+}__attribute__((packed));
+
+struct hotcpu_event {
+  __u8 cpu;
+}__attribute__((packed));
+
+struct cpufreq_set_event {
+  __u8 cpu;
+  __le32 old_freq;
+  __le32 new_freq;
+}__attribute__((packed));
+
+struct wake_lock_event {
+  __le32 lock;
+  __le32 timeout;
+}__attribute__((packed));
+
+struct wake_unlock_event {
+  __le32 lock;
+}__attribute__((packed));
+
+struct fork_event {
+  __le16 pid;
+  __le16 tgid;
+}__attribute__((packed));
+
+struct thread_name_event {
+  __u16 pid;
+  char comm[16];
+}__attribute__((packed));
+
+struct general_lock_event {
+  __le32 lock;
+}__attribute__((packed));
+
+struct general_notify_event {
+  __le32 lock;
+  __le16 pid;
+}__attribute__((packed));
+
+struct binder_event {
+  __le32 transaction;
+}__attribute__((packed));
+
+struct cpufreq_mod_timer_event {
+  __u8 cpu;
+  __u32 microseconds;
+}__attribute__((packed));
+
+struct cpufreq_timer_event {
+  __u8 cpu;
+}__attribute__((packed));
+
+struct simple_event {
+}__attribute__((packed));
+
+#ifdef __KERNEL__
+
+#include <linux/hardirq.h>
+#include <linux/time.h>
+#include <linux/sched.h>
+#include <linux/smp.h>
+
+#ifdef CONFIG_EVENT_LOGGING
+extern void* reserve_event(int len);
+extern void shrink_event(int len);
+extern void poke_queues(void);
+extern struct timeval* get_timestamp(void);
+
+#define __init_event(type, event_type, name, diff)			\
+  struct timeval tv;							\
+  u8 sec_len;								\
+  u8 usec_len;								\
+  struct event_hdr* header;						\
+  char* sec;								\
+  char* usec;								\
+  type* name;								\
+  unsigned long flags;							\
+  local_irq_save(flags);						\
+  header = (typeof(header)) reserve_event(sizeof(*header) + 4 + 3 + sizeof(*name)); \
+  if (header) {								\
+  tv = event_log_timestamp(diff);					\
+  if (diff) {								\
+    sec_len = vsize_sec(tv.tv_sec);					\
+    usec_len = vsize_usec(tv.tv_usec);					\
+  } else {								\
+    sec_len = 4;							\
+    usec_len = 3;							\
+  }									\
+  shrink_event(4 + 3 - sec_len - usec_len);				\
+  sec = (char*) (header+1);						\
+  usec = sec + sec_len;							\
+  name = (typeof(name)) (usec + usec_len);				\
+  memcpy(sec, &tv.tv_sec, sec_len);					\
+  memcpy(usec, &tv.tv_usec, usec_len);					\
+  event_log_header_init(header, sec_len, usec_len, event_type)
+
+#define init_event(type, event_type, name) __init_event(type, event_type, name, 1)
+
+#define finish_event() poke_queues(); \
+  }				      \
+ local_irq_restore(flags)
+
+#define finish_event_no_poke() }      \
+    local_irq_restore(flags)
+
+/* Records the current timestamp and, if diff is true, returns the
+ * time passed since the last timestamp. Otherwise, the recorded current
+ * time is returned.  static inline struct timeval */
+static inline struct timeval event_log_timestamp(int diff) {
+  struct timeval *cur = get_timestamp();
+  struct timeval prev = *cur;
+  do_gettimeofday(cur);
+  if (!diff) 
+    return *cur;
+  prev.tv_sec = cur->tv_sec - prev.tv_sec;
+  prev.tv_usec = cur->tv_usec - prev.tv_usec;
+  return prev;
+}
+
+static inline u8 vsize_usec(long val) {
+  if ((MIN8 <= val) && (val <= MAX8))
+    return 1;
+  else if ((MIN16 <= val) && (val <= MAX16))
+    return 2;
+  else
+    return 3;
+}
+
+static inline u8 vsize_sec(long val) {
+  if (val == 0)
+    return 0;
+  else if ((MIN8 <= val) && (val <= MAX8))
+    return 1;
+  else if ((MIN16 <= val) && (val <= MAX16))
+    return 2;
+  else if ((MIN24 <= val) && (val <= MAX24))
+    return 3;
+  else
+    return 4;
+}
+
+static inline void event_log_header_init(struct event_hdr* event, u8 sec_len, u8 usec_len, u8 type) {
+  event->event_type = type;
+  SET_CPU(event, smp_processor_id());
+  SET_TVLEN(event, sec_len, usec_len);
+  event->pid = current->pid | (in_interrupt() ? 0x8000 : 0);
+}
+
+static inline void event_log_simple(u8 event_type) {
+  init_event(struct simple_event, event_type, event);
+  finish_event();
+}
+
+static inline void event_log_simple_no_poke(u8 event_type) {
+  init_event(struct simple_event, event_type, event);
+  finish_event_no_poke();
+}
+
+static inline void event_log_sync(void) {
+  __init_event(struct sync_log_event, EVENT_SYNC_LOG, event, 0);
+  memcpy(&event->magic, EVENT_LOG_MAGIC, 8);
+  finish_event_no_poke();
+}
+
+static inline void event_log_missed_count(int* count) {
+  init_event(struct missed_count_event, EVENT_MISSED_COUNT, event);
+  event->count = *count;
+  *count = 0;
+  finish_event_no_poke();
+}
+
+static inline void event_log_general_lock(__u8 event_type, void* lock) {
+  init_event(struct general_lock_event, event_type, event);
+  event->lock = (__le32) lock;
+  finish_event_no_poke();
+}
+
+static inline void event_log_general_notify(__u8 event_type, void* lock, pid_t pid) {
+  init_event(struct general_notify_event, event_type, event);
+  event->lock = (__le32) lock;
+  event->pid = pid;
+  finish_event_no_poke();
+}
+
+#endif
+
+static inline void event_log_context_switch(pid_t new, long state) {
+#ifdef CONFIG_EVENT_CONTEXT_SWITCH
+  init_event(struct context_switch_event, EVENT_CONTEXT_SWITCH, event);
+  event->new_pid = new;
+  event->state = (__u8) (0x0FF & state);
+  finish_event_no_poke();
+#endif
+}
+
+static inline void event_log_preempt_tick(void) {
+#ifdef CONFIG_EVENT_PREEMPT_TICK
+  event_log_simple_no_poke(EVENT_PREEMPT_TICK);
+#endif
+}
+
+static inline void event_log_preempt_wakeup(void) {
+#ifdef CONFIG_EVENT_PREEMPT_WAKEUP
+  event_log_simple_no_poke(EVENT_PREEMPT_WAKEUP);
+#endif
+}
+
+static inline void event_log_yield(void) {
+#ifdef CONFIG_EVENT_YIELD
+  event_log_simple(EVENT_YIELD);
+#endif
+}
+
+#if defined(CONFIG_EVENT_CPU_ONLINE) || defined(CONFIG_EVENT_CPU_DEAD) || defined(CONFIG_EVENT_CPU_DOWN_PREPARE)
+static inline void event_log_hotcpu(u8 event_type, unsigned int cpu) {
+  init_event(struct hotcpu_event, event_type, event);
+  event->cpu = cpu;
+  finish_event();
+}
+#endif
+
+static inline void event_log_cpu_online(unsigned int cpu) {
+#ifdef CONFIG_EVENT_CPU_ONLINE
+  event_log_hotcpu(EVENT_CPU_ONLINE, cpu);
+#endif
+}
+
+static inline void event_log_cpu_down_prepare(unsigned int cpu) {
+#ifdef CONFIG_EVENT_CPU_DOWN_PREPARE
+  event_log_hotcpu(EVENT_CPU_DOWN_PREPARE, cpu);
+#endif
+}
+
+static inline void event_log_cpu_dead(unsigned int cpu) {
+#ifdef CONFIG_EVENT_CPU_DEAD
+  event_log_hotcpu(EVENT_CPU_DEAD, cpu);
+#endif
+}
+
+#if defined(CONFIG_EVENT_BINDER_PRODUCE_ONEWAY) || defined(CONFIG_EVENT_BINDER_PRODUCE_TWOWAY) \
+ || defined(CONFIG_EVENT_BINDER_PRODUCE_REPLY) || defined(CONFIG_EVENT_BINDER_CONSUME)
+static inline void event_log_binder(u8 event_type, void* transaction) {
+  init_event(struct binder_event, event_type, event);
+  event->transaction = (__le32) transaction;
+  finish_event();
+}
+#endif
+
+static inline void event_log_binder_produce_oneway(void* transaction) {
+  #ifdef CONFIG_EVENT_BINDER_PRODUCE_ONEWAY
+  event_log_binder(EVENT_BINDER_PRODUCE_ONEWAY, transaction);
+  #endif
+}
+
+static inline void event_log_binder_produce_twoway(void* transaction) {
+  #ifdef CONFIG_EVENT_BINDER_PRODUCE_TWOWAY
+  event_log_binder(EVENT_BINDER_PRODUCE_TWOWAY, transaction);
+  #endif
+}
+
+static inline void event_log_binder_produce_reply(void* transaction) {
+  #ifdef CONFIG_EVENT_BINDER_PRODUCE_REPLY
+  event_log_binder(EVENT_BINDER_PRODUCE_REPLY, transaction);
+  #endif
+}
+
+static inline void event_log_binder_consume(void* transaction) {
+  #ifdef CONFIG_EVENT_BINDER_CONSUME
+  event_log_binder(EVENT_BINDER_CONSUME, transaction);
+  #endif
+}
+
+static inline void event_log_cpufreq_set(unsigned int cpu, unsigned int old_freq, unsigned int new_freq) {
+#ifdef CONFIG_EVENT_CPUFREQ_SET
+  init_event(struct cpufreq_set_event, EVENT_CPUFREQ_SET, event);
+  event->cpu = cpu;
+  event->old_freq = old_freq;
+  event->new_freq = new_freq;
+  finish_event();
+#endif
+}
+
+static inline void event_log_idle_start(void) {
+#ifdef CONFIG_EVENT_IDLE_START
+  event_log_simple(EVENT_IDLE_START);
+#endif
+}
+
+static inline void event_log_idle_end(void) {
+#ifdef CONFIG_EVENT_IDLE_END
+  event_log_simple(EVENT_IDLE_END);
+#endif
+}
+
+static inline void event_log_suspend_start(void) {
+#ifdef CONFIG_EVENT_SUSPEND_START
+  event_log_simple_no_poke(EVENT_SUSPEND_START);
+#endif
+}
+
+static inline void event_log_suspend(void) {
+#ifdef CONFIG_EVENT_SUSPEND
+  event_log_simple_no_poke(EVENT_SUSPEND);
+#endif
+}
+
+static inline void event_log_resume(void) {
+#ifdef CONFIG_EVENT_RESUME
+  event_log_simple_no_poke(EVENT_RESUME);
+#endif
+}
+
+static inline void event_log_resume_finish(void) {
+#ifdef CONFIG_EVENT_RESUME_FINISH
+  event_log_simple_no_poke(EVENT_RESUME_FINISH);
+#endif
+}
+
+static inline void event_log_datagram_block(void) {
+#ifdef CONFIG_EVENT_DATAGRAM_BLOCK
+  event_log_simple(EVENT_DATAGRAM_BLOCK);
+#endif
+}
+
+static inline void event_log_datagram_resume(void) {
+#ifdef CONFIG_EVENT_DATAGRAM_RESUME
+  event_log_simple(EVENT_DATAGRAM_RESUME);
+#endif
+}
+
+static inline void event_log_stream_block(void) {
+#ifdef CONFIG_EVENT_STREAM_BLOCK
+  event_log_simple(EVENT_STREAM_BLOCK);
+#endif
+}
+
+static inline void event_log_stream_resume(void) {
+#ifdef CONFIG_EVENT_STREAM_RESUME
+  event_log_simple(EVENT_STREAM_RESUME);
+#endif
+}
+
+static inline void event_log_sock_block(void) {
+#ifdef CONFIG_EVENT_SOCK_BLOCK
+  event_log_simple(EVENT_SOCK_BLOCK);
+#endif
+}
+
+static inline void event_log_sock_resume(void) {
+#ifdef CONFIG_EVENT_SOCK_RESUME
+  event_log_simple(EVENT_SOCK_RESUME);
+#endif
+}
+
+static inline void event_log_io_block(void) {
+#ifdef CONFIG_EVENT_IO_BLOCK
+  event_log_simple(EVENT_IO_BLOCK);
+#endif
+}
+
+static inline void event_log_io_resume(void) {
+#ifdef CONFIG_EVENT_IO_RESUME
+  event_log_simple(EVENT_IO_RESUME);
+#endif
+}
+
+static inline void event_log_wake_lock(void* lock, long timeout) {
+#ifdef CONFIG_EVENT_WAKE_LOCK
+  init_event(struct wake_lock_event, EVENT_WAKE_LOCK, event);
+  event->lock = (__le32) lock;
+  event->timeout = timeout;
+  finish_event();
+#endif
+}
+
+static inline void event_log_wake_unlock(void* lock) {
+#ifdef CONFIG_EVENT_WAKE_UNLOCK
+  init_event(struct wake_unlock_event, EVENT_WAKE_UNLOCK, event);
+  event->lock = (__le32) lock;
+  finish_event();
+#endif
+}
+
+static inline void event_log_fork(pid_t pid, pid_t tgid) {
+#ifdef CONFIG_EVENT_FORK
+  init_event(struct fork_event, EVENT_FORK, event);
+  event->pid = pid;
+  event->tgid = tgid;
+  finish_event();
+#endif
+}
+
+static inline void event_log_exit(void) {
+#ifdef CONFIG_EVENT_EXIT
+  event_log_simple(EVENT_EXIT);
+#endif
+}
+
+static inline void event_log_thread_name(struct task_struct* task) {
+#ifdef CONFIG_EVENT_THREAD_NAME
+  init_event(struct thread_name_event, EVENT_THREAD_NAME, event);
+  event->pid = task->pid;
+  memcpy(event->comm, task->comm, min(16, TASK_COMM_LEN));
+  finish_event(); 
+#endif
+}
+
+/* Can't be inlined due to #include ordering conflicts in wait.h and I
+ * don't want to figure that out right now.  Can do it later, but
+ * might involve a separate header file for the waitqueue events
+ * or externing (instead of #including) the depedencies
+ * for event_log_header_init().
+ */
+void event_log_waitqueue_wait(void* wq);
+void event_log_waitqueue_wake(void* wq);
+void event_log_waitqueue_notify(void* wq, pid_t pid);
+
+static inline void event_log_mutex_lock(void* lock) {
+#ifdef CONFIG_EVENT_MUTEX_LOCK
+  event_log_general_lock(EVENT_MUTEX_LOCK, lock);
+#endif
+}
+
+static inline void event_log_mutex_wait(void* lock) {
+#ifdef CONFIG_EVENT_MUTEX_WAIT
+  event_log_general_lock(EVENT_MUTEX_WAIT, lock);
+#endif
+}
+
+static inline void event_log_mutex_wake(void* lock) {
+#ifdef CONFIG_EVENT_MUTEX_WAKE
+  event_log_general_lock(EVENT_MUTEX_WAKE, lock);
+#endif
+}
+
+static inline void event_log_mutex_notify(void* lock, pid_t pid) {
+#ifdef CONFIG_EVENT_MUTEX_NOTIFY
+  event_log_general_notify(EVENT_MUTEX_NOTIFY, lock, pid);
+#endif
+}
+
+static inline void event_log_futex_wait(void* lock) {
+#ifdef CONFIG_EVENT_FUTEX_WAIT
+  event_log_general_lock(EVENT_FUTEX_WAIT, lock);
+#endif
+}
+
+static inline void event_log_futex_wake(void* lock) {
+#ifdef CONFIG_EVENT_MUTEX_WAKE
+  event_log_general_lock(EVENT_FUTEX_WAKE, lock);
+#endif
+}
+
+static inline void event_log_futex_notify(void* lock, pid_t pid) {
+#ifdef CONFIG_EVENT_FUTEX_NOTIFY
+  event_log_general_notify(EVENT_FUTEX_NOTIFY, lock, pid);
+#endif
+}
+
+static inline void event_log_sem_lock(void* lock) {
+#ifdef CONFIG_EVENT_SEMAPHORE_LOCK
+  event_log_general_lock(EVENT_SEMAPHORE_LOCK, lock);
+#endif
+}
+
+static inline void event_log_sem_wait(void* lock) {
+#ifdef CONFIG_EVENT_SEMAPHORE_WAIT
+  event_log_general_lock(EVENT_SEMAPHORE_WAIT, lock);
+#endif
+}
+
+static inline void event_log_sem_wake(void* lock) {
+#ifdef CONFIG_EVENT_SEMAPHORE_WAIT
+  event_log_general_lock(EVENT_SEMAPHORE_WAIT, lock);
+#endif
+}
+
+static inline void event_log_sem_notify(void* lock, pid_t pid) {
+#ifdef CONFIG_EVENT_SEMAPHORE_NOTIFY
+  event_log_general_notify(EVENT_SEMAPHORE_NOTIFY, lock, pid);
+#endif
+}
+
+static inline void event_log_cpufreq_boost(void) {
+#ifdef CONFIG_EVENT_CPUFREQ_BOOST
+  event_log_simple(EVENT_CPUFREQ_BOOST);
+#endif
+}
+
+static inline void event_log_cpufreq_wake_up(void) {
+#ifdef CONFIG_EVENT_CPUFREQ_WAKE_UP
+  event_log_simple(EVENT_CPUFREQ_WAKE_UP);
+#endif
+}
+
+static inline void event_log_cpufreq_mod_timer(unsigned int cpu, unsigned int microseconds) {
+#ifdef CONFIG_EVENT_CPUFREQ_MOD_TIMER
+  init_event(struct cpufreq_mod_timer_event, EVENT_CPUFREQ_MOD_TIMER, event);
+  event->cpu = cpu;
+  event->microseconds = microseconds;
+  finish_event(); 
+#endif
+}
+
+static inline void event_log_cpufreq_del_timer(unsigned int cpu) {
+#ifdef CONFIG_EVENT_CPUFREQ_DEL_TIMER
+  init_event(struct cpufreq_timer_event, EVENT_CPUFREQ_DEL_TIMER, event);
+  event->cpu = cpu;
+  finish_event(); 
+#endif
+}
+
+static inline void event_log_cpufreq_timer(unsigned int cpu) {
+#ifdef CONFIG_EVENT_CPUFREQ_TIMER
+  init_event(struct cpufreq_timer_event, EVENT_CPUFREQ_TIMER, event);
+  event->cpu = cpu;
+  finish_event(); 
+#endif
+}
+
+#endif // __KERNEL__
+#endif // EVENTLOGGING_EVENTS_H
diff --git a/include/linux/wait.h b/include/linux/wait.h
index 3efc9f3..d16a044 100644
--- a/include/linux/wait.h
+++ b/include/linux/wait.h
@@ -25,6 +25,24 @@
 #include <asm/system.h>
 #include <asm/current.h>
 
+#ifdef CONFIG_EVENT_WAITQUEUE_WAIT
+extern void event_log_waitqueue_wait(void* wq);
+#else
+#define event_log_waitqueue_wait(t) do{;}while(0);
+#endif
+
+#ifdef CONFIG_EVENT_WAITQUEUE_WAKE
+extern void event_log_waitqueue_wake(void* wq);
+#else
+#define event_log_waitqueue_wake(t) do{;}while(0);
+#endif
+
+#ifdef CONFIG_EVENT_WAITQUEUE_NOTIFY
+extern void event_log_waitqueue_notify(void* wq, pid_t pid);
+#else
+#define event_log_waitqueue_notify(t, p) do{;}while(0);
+#endif
+
 typedef struct __wait_queue wait_queue_t;
 typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
 int default_wake_function(wait_queue_t *wait, unsigned mode, int flags, void *key);
@@ -395,7 +413,9 @@ do {									\
 			spin_unlock_irq(&(wq).lock);			\
 		else							\
 			spin_unlock(&(wq).lock);			\
+		event_log_waitqueue_wait(&wq);				\
 		schedule();						\
+		event_log_waitqueue_wake(&wq);				\
 		if (irq)						\
 			spin_lock_irq(&(wq).lock);			\
 		else							\
diff --git a/kernel/Makefile b/kernel/Makefile
index e4fd98b..d813ba8 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -73,6 +73,7 @@ obj-$(CONFIG_AUDITSYSCALL) += auditsc.o
 obj-$(CONFIG_AUDIT_WATCH) += audit_watch.o
 obj-$(CONFIG_AUDIT_TREE) += audit_tree.o
 obj-$(CONFIG_GCOV_KERNEL) += gcov/
+obj-$(CONFIG_EVENT_LOGGING) += eventlogging/
 obj-$(CONFIG_KPROBES) += kprobes.o
 obj-$(CONFIG_KGDB) += debug/
 obj-$(CONFIG_DETECT_HUNG_TASK) += hung_task.o
diff --git a/kernel/eventlogging/Kconfig b/kernel/eventlogging/Kconfig
new file mode 100644
index 0000000..0150186
--- /dev/null
+++ b/kernel/eventlogging/Kconfig
@@ -0,0 +1,215 @@
+#
+# Event logging framework for Lide Zhang and David Bild.
+#
+#
+
+menuconfig EVENT_LOGGING
+        bool "Event Logging Zhang/Bild"
+	select LZO_COMPRESS
+        help
+          Event tracing framework for Lide Zhang and David Bild.
+
+if EVENT_LOGGING
+
+config EVENT_CPU_ONLINE
+       bool "Log when cpu comes online"
+       default yes
+
+config EVENT_CPU_DOWN_PREPARE
+       bool "Log when cpu prepares to go offline"
+       default yes
+
+config EVENT_CPU_DEAD
+       bool "Log when cpu goes offline"
+       default yes
+
+config EVENT_CPUFREQ_SET
+       bool "Log when cpu frequency changes"
+       default yes
+
+config EVENT_BINDER_PRODUCE_ONEWAY
+       bool "Log when a one-way binder transaction is submitted"
+       default yes
+
+config EVENT_BINDER_PRODUCE_TWOWAY
+       bool "Log when a two-way binder transaction is submitted"
+       default yes
+
+config EVENT_BINDER_PRODUCE_REPLY
+       bool "Log when a reply transaction is submitted"
+       default yes
+
+config EVENT_BINDER_CONSUME
+       bool "Log when a  binder transaction is processed"
+       default yes
+
+config EVENT_SUSPEND_START
+       bool "Log when a suspend request is issued"
+       default yes
+
+config EVENT_SUSPEND
+       bool "Log when the system enters suspend"
+       default yes
+
+config EVENT_RESUME
+       bool "Log when the system resumes"
+       default yes
+
+config EVENT_RESUME_FINISH
+       bool "Log when the resume has finished"
+       default yes
+
+config EVENT_WAITQUEUE_WAIT
+       bool "Log wait queue event waits"
+       default yes
+
+config EVENT_WAITQUEUE_WAKE
+       bool "Log wait queue event wakes"
+       default yes
+
+config EVENT_WAITQUEUE_NOTIFY
+       bool "Log wait queue event notifies"
+       default yes
+
+config EVENT_WAKE_LOCK
+       bool "Log when a kernel wakelock is acquired"
+       default yes
+
+config EVENT_WAKE_UNLOCK
+       bool "Log when a kernel wakelock is released"
+       default yes
+
+config EVENT_CONTEXT_SWITCH
+       bool "Log context switches"
+       default no
+
+config EVENT_PREEMPT_TICK
+       bool "Log when a process is about to be preempted due to timeout"
+       default yes
+
+config EVENT_PREEMPT_WAKEUP
+       bool "Log when a process is about to be preempted due to wakeup"
+       default yes
+
+config EVENT_YIELD
+       bool "Log when a process yields the CPU"
+       default yes
+
+config EVENT_IDLE_START
+       bool "Log when idle loop starts"
+       default yes
+
+config EVENT_IDLE_END
+       bool "Log when idle loop stops"
+       default yes
+
+config EVENT_FORK
+       bool "Log forks"
+       default yes
+
+config EVENT_EXIT
+       bool "Log when process exits"
+       default yes
+
+config EVENT_IO_BLOCK
+       bool "Log IO blocks"
+       default yes
+
+config EVENT_IO_RESUME
+       bool "Log IO resumes"
+       default yes
+
+config EVENT_DATAGRAM_BLOCK
+       bool "Log datagram blocks"
+       default yes
+
+config EVENT_DATAGRAM_RESUME
+       bool "Log datagram resumes"
+       default yes
+
+config EVENT_STREAM_BLOCK
+       bool "Log stream blocks"
+       default yes
+
+config EVENT_STREAM_RESUME
+       bool "Log stream resumes"
+       default yes
+
+config EVENT_SOCK_BLOCK
+       bool "Log socket blocks"
+       default yes
+
+config EVENT_SOCK_RESUME
+       bool "Log socket resumes"
+       default yes
+
+config EVENT_SEMAPHORE_LOCK
+       bool "Log semaphore locks"
+       default no
+
+config EVENT_SEMAPHORE_WAIT
+       bool "Log semaphore waits"
+       default yes
+
+config EVENT_SEMAPHORE_WAKE
+       bool "Log semaphore wakes"
+       default yes
+
+config EVENT_SEMAPHORE_NOTIFY
+       bool "Log semaphore notifies"
+       default yes
+
+config EVENT_MUTEX_LOCK
+       bool "Log mutex locks"
+       default no
+
+config EVENT_MUTEX_WAIT
+       bool "Log mutex waits"
+       default yes
+
+config EVENT_MUTEX_WAKE
+       bool "Log mutex wake"
+       default yes
+
+config EVENT_MUTEX_NOTIFY
+       bool "Log who wakes up a mutex"
+       default yes
+
+config EVENT_FUTEX_WAIT
+       bool "Log futex waits"
+       default yes
+
+config EVENT_FUTEX_WAKE
+       bool "Log futex wakeups"
+       default yes
+
+config EVENT_FUTEX_NOTIFY
+       bool "Log who wakes up a futex"
+       default yes
+
+config EVENT_THREAD_NAME
+       bool "Log when a process name is changed"
+       default yes
+
+config EVENT_CPUFREQ_BOOST
+       bool "Log when boost is called, presumably after a input event"
+       default yes
+
+config EVENT_CPUFREQ_WAKE_UP
+       bool "Log when thread kinteractiveup is woken up"
+       default yes
+
+config EVENT_CPUFREQ_MOD_TIMER
+       bool "Log when governor time is set"
+       default yes
+
+config EVENT_CPUFREQ_DEL_TIMER
+	bool "Log when governor timer is deleted"
+	default yes
+
+config EVENT_CPUFREQ_TIMER
+       bool "Log when governor timer callback is executed"
+       default yes
+
+endif
+
diff --git a/kernel/eventlogging/Makefile b/kernel/eventlogging/Makefile
new file mode 100644
index 0000000..e023784
--- /dev/null
+++ b/kernel/eventlogging/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_EVENT_LOGGING) := logging.o buffer.o idle.o hotcpu.o cpufreq.o events.o
diff --git a/kernel/eventlogging/buffer.c b/kernel/eventlogging/buffer.c
new file mode 100644
index 0000000..0053560
--- /dev/null
+++ b/kernel/eventlogging/buffer.c
@@ -0,0 +1,105 @@
+#include <linux/bootmem.h>
+#include <linux/sched.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+
+#include "buffer.h"
+
+// max order is likely 10 or 11
+int sbuffer_init(struct sbuffer* buf, unsigned int order) {
+  void* addr;
+
+  addr = (void*) __get_free_pages(GFP_ATOMIC, order);
+  if (!addr)
+    goto err;
+
+  INIT_LIST_HEAD(&buf->list);
+  buf->order = order;
+  buf->start = addr;
+  buf->end   = addr + (1 << order) * PAGE_SIZE;
+  buf->rp    = addr;
+  buf->wp    = addr;
+  return 0;
+
+ err:
+  return -ENOMEM;
+}
+
+void sbuffer_free(struct sbuffer* buf) {
+  free_pages((unsigned long) buf->start, buf->order);
+}
+
+void sbuffer_clear(struct sbuffer* buf) {
+  buf->rp = buf->start;
+  buf->wp = buf->start;
+}
+
+/* Returns NULL if too full */
+void* sbuffer_reserve(struct sbuffer* buf, int len) {
+  void* old_wp = buf->wp;
+  if (buf->wp + len <= buf-> end) {
+    buf->wp += len;
+    return old_wp;
+  } else {
+    return NULL;
+  }
+}
+
+/* Returns the specified number of bytes to the buffer */
+void sbuffer_cancel(struct sbuffer* buf, int len) {
+  buf->wp -= len;
+}
+
+int sbuffer_empty(struct sbuffer* buf) {
+  return (buf->rp == buf->wp);
+}
+
+int sbuffer_read(struct sbuffer* buf, char* page, int count) {
+  int len, num;
+  int avail;
+  
+  len = 0;
+  avail = buf->wp - buf->rp;
+  num = min(count, avail);
+
+  if (num > 0) {
+    memcpy(page, buf->rp, num);
+    buf->rp += num;
+    len += num;
+  }
+
+  return len;
+}
+
+void sbuffer_restart_read(struct sbuffer* buf) {
+  buf->rp = buf->start;
+}
+
+/* Swaps the memory held by the two buffers */
+void sbuffer_swap(struct sbuffer* buf1, struct sbuffer* buf2) {
+  int order;
+  void *start, *end, *rp, *wp;
+  
+  /* Save values from buf1 */
+  order = buf1->order;
+  start = buf1->start;
+  end = buf1->end;
+  rp = buf1->rp;
+  wp = buf1->wp;
+  
+  /* Move buf2 to buf1 */
+  buf1->order = buf2->order;
+  buf1->start = buf2->start;
+  buf1->end = buf2->end;
+  buf1->rp = buf2->rp;
+  buf1->wp = buf2->wp;
+
+  /* And move buf1 copy to buf2 */
+  buf2->order = order;
+  buf2->start = start;
+  buf2->end = end;
+  buf2->rp = rp;
+  buf2->wp = wp;
+}
+
diff --git a/kernel/eventlogging/buffer.h b/kernel/eventlogging/buffer.h
new file mode 100644
index 0000000..e9d1f68
--- /dev/null
+++ b/kernel/eventlogging/buffer.h
@@ -0,0 +1,32 @@
+#ifndef EVENT_LOGGING_BUFFER_H
+#define EVENT_LOGGING_BUFFER_H
+
+#include <linux/list.h>
+#include <linux/workqueue.h>
+
+struct sbuffer {
+  struct list_head list;
+  struct work_struct work;
+  int order;   // order of page allocation (2^order pages)
+  void* start; // starting address
+  void* end;   // last address in buffer
+  void* rp;    // pointer to next byte to read
+  void* wp;    // pointer to next byte writer
+};
+
+void sbuffer_print_empty(void);
+
+int sbuffer_init(struct sbuffer* buf, unsigned int order);
+void sbuffer_free(struct sbuffer* buf);
+
+void sbuffer_clear(struct sbuffer* buf);
+void* sbuffer_reserve(struct sbuffer* buf, int len);
+void sbuffer_cancel(struct sbuffer* buf, int len);
+int sbuffer_empty(struct sbuffer* buf); // any data to read?
+int sbuffer_read(struct sbuffer* buf, char* page, int count);
+void sbuffer_restart_read(struct sbuffer* buf);
+
+/* Swaps the memory held by the two buffers */
+void sbuffer_swap(struct sbuffer* buf1, struct sbuffer* buf2); 
+
+#endif
diff --git a/kernel/eventlogging/cpufreq.c b/kernel/eventlogging/cpufreq.c
new file mode 100644
index 0000000..92303ca
--- /dev/null
+++ b/kernel/eventlogging/cpufreq.c
@@ -0,0 +1,29 @@
+#include <linux/cpufreq.h>
+#include <linux/notifier.h>
+
+#include <eventlogging/events.h>
+
+#ifdef CONFIG_EVENT_CPUFREQ_SET
+static struct notifier_block cpufreq_notifier;
+
+static int cpufreq_notifier_call(struct notifier_block* self, unsigned long event, void* data) {
+  struct cpufreq_freqs* freqs;
+  switch (event) {
+  case CPUFREQ_POSTCHANGE:
+    freqs = (struct cpufreq_freqs*) data;
+    event_log_cpufreq_set(freqs->cpu, freqs->old, freqs->new);
+    break;
+  default:
+    break;
+  }
+  return 0;
+}
+
+__init int init_cpufreq_notifier(void) {
+  cpufreq_notifier.notifier_call = cpufreq_notifier_call;
+  cpufreq_register_notifier(&cpufreq_notifier, CPUFREQ_TRANSITION_NOTIFIER);
+  return 0;
+}
+#else
+__init int init_cpufreq_notifier(void) {return 0;}
+#endif
diff --git a/kernel/eventlogging/cpufreq.h b/kernel/eventlogging/cpufreq.h
new file mode 100644
index 0000000..916dbd6
--- /dev/null
+++ b/kernel/eventlogging/cpufreq.h
@@ -0,0 +1,6 @@
+#ifndef EVENT_LOGGING_CPUFREQ_H
+#define EVENT_LOGGING_CPUFREQ_H
+
+__init int init_cpufreq_notifier(void);
+
+#endif
diff --git a/kernel/eventlogging/events.c b/kernel/eventlogging/events.c
new file mode 100644
index 0000000..c189332
--- /dev/null
+++ b/kernel/eventlogging/events.c
@@ -0,0 +1,19 @@
+#include <eventlogging/events.h>
+
+void event_log_waitqueue_wait(void* wq) {
+#ifdef CONFIG_EVENT_WAITQUEUE_WAIT
+  event_log_general_lock(EVENT_WAITQUEUE_WAIT, wq);
+#endif
+}
+
+void event_log_waitqueue_wake(void* wq) {
+#ifdef CONFIG_EVENT_WAITQUEUE_WAKE
+  event_log_general_lock(EVENT_WAITQUEUE_WAKE, wq);
+#endif
+}
+
+void event_log_waitqueue_notify(void* wq, pid_t pid) {
+#ifdef CONFIG_EVENT_WAITQUEUE_NOTIFY
+  event_log_general_notify(EVENT_WAITQUEUE_NOTIFY, wq, pid);
+#endif
+}
diff --git a/kernel/eventlogging/hotcpu.c b/kernel/eventlogging/hotcpu.c
new file mode 100644
index 0000000..ade6a24
--- /dev/null
+++ b/kernel/eventlogging/hotcpu.c
@@ -0,0 +1,40 @@
+#include <linux/cpu.h>
+#include <linux/notifier.h>
+
+#include <eventlogging/events.h>
+
+#if defined(CONFIG_EVENT_CPU_ONLINE) || defined(CONFIG_EVENT_CPU_DEAD) || defined(CONFIG_EVENT_CPU_DOWN_PREPARE)
+
+static int hotcpu_notifier_call(struct notifier_block* self, unsigned long event, void* hcpu) {
+  unsigned int cpu = (unsigned long) hcpu;
+
+  switch (event) {
+  case CPU_ONLINE:
+  case CPU_ONLINE_FROZEN:
+    event_log_cpu_online(cpu);
+    break;
+  case CPU_DOWN_PREPARE:
+  case CPU_DOWN_PREPARE_FROZEN:
+    event_log_cpu_down_prepare(cpu);
+    break;
+  case CPU_DEAD:
+  case CPU_DEAD_FROZEN:
+    event_log_cpu_dead(cpu);
+    break;
+  }
+  return NOTIFY_OK;
+}
+
+static struct notifier_block hotcpu_notifier = {
+  .notifier_call = hotcpu_notifier_call
+};
+
+__init int init_hotcpu_notifier(void) {
+  register_cpu_notifier(&hotcpu_notifier);
+  return 0;
+}
+#else
+__init int init_hotcpu_notifier(void){
+  return 0;
+}
+#endif
diff --git a/kernel/eventlogging/hotcpu.h b/kernel/eventlogging/hotcpu.h
new file mode 100644
index 0000000..1daa1ac
--- /dev/null
+++ b/kernel/eventlogging/hotcpu.h
@@ -0,0 +1,6 @@
+#ifndef EVENT_LOGGING_HOTCPU_H
+#define EVENT_LOGGING_HOTCPU_H
+
+__init int init_hotcpu_notifier(void);
+
+#endif
diff --git a/kernel/eventlogging/idle.c b/kernel/eventlogging/idle.c
new file mode 100644
index 0000000..60074a1
--- /dev/null
+++ b/kernel/eventlogging/idle.c
@@ -0,0 +1,28 @@
+#include <linux/cpu.h>
+#include <linux/notifier.h>
+
+#include <eventlogging/events.h>
+
+#if defined(CONFIG_EVENT_IDLE_START) || defined(CONFIG_EVENT_IDLE_STOP)
+static struct notifier_block idle_notifier;
+
+static int idle_notifier_call(struct notifier_block* self, unsigned long event, void* data) {
+  switch (event) {
+  case IDLE_START:
+    event_log_idle_start();
+    break;
+  case IDLE_END:
+    event_log_idle_end();
+    break;
+  }
+  return 0;
+}
+
+__init int init_idle_notifier(void) {
+  idle_notifier.notifier_call = idle_notifier_call;
+  idle_notifier_register(&idle_notifier);
+  return 0;
+}
+#else
+__init int init_idle_notifier(void) {return 0;}
+#endif
diff --git a/kernel/eventlogging/idle.h b/kernel/eventlogging/idle.h
new file mode 100644
index 0000000..503c1d9
--- /dev/null
+++ b/kernel/eventlogging/idle.h
@@ -0,0 +1,6 @@
+#ifndef EVENT_LOGGING_IDLE_H
+#define EVENT_LOGGING_IDLE_H
+
+__init int init_idle_notifier(void);
+
+#endif
diff --git a/kernel/eventlogging/logging.c b/kernel/eventlogging/logging.c
new file mode 100644
index 0000000..40d2e4f
--- /dev/null
+++ b/kernel/eventlogging/logging.c
@@ -0,0 +1,433 @@
+#include <linux/cpumask.h>
+#include <linux/percpu.h>
+#include <linux/smp.h>
+#include <linux/cpu.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/lzo.h>
+#include <linux/string.h>
+
+#include <asm/uaccess.h>
+
+#include <eventlogging/events.h>
+
+#include "logging.h"
+#include "buffer.h"
+#include "idle.h"
+#include "hotcpu.h"
+#include "cpufreq.h"
+#include "queue.h"
+
+#define BUFFER_ORDER 10  // 2^10 = 4 MB with 4096 page size
+#define NUM_BUFFERS   8  // 8 * 4 MB = 32 MB total
+
+static DEFINE_PER_CPU(struct sbuffer*, cpu_buffers);
+static DEFINE_PER_CPU(unsigned int, missed_events);
+
+/* Timestamp from last packet */
+static DEFINE_PER_CPU(struct timeval, last_tv);
+
+static DEFINE_QUEUE(empty_buffers);
+static DEFINE_QUEUE(full_buffers);
+static DEFINE_QUEUE(compressed_buffers);
+
+#define PFS_NAME "event_logging"
+#define PFS_COMMAND_LEN 10
+#define PFS_RESTART "restart"
+#define PFS_CLEAR "clear"
+#define PFS_PERMS S_IFREG|S_IROTH|S_IRGRP|S_IRUSR|S_IWOTH|S_IWGRP|S_IWUSR
+static struct proc_dir_entry* el_pfs_entry;
+static DEFINE_MUTEX(pfs_read_lock);
+static struct sbuffer* pfs_read_buffer;
+
+static DEFINE_MUTEX(compress_lock);
+static struct sbuffer* compress_empty_buffer;
+
+static void init_new_buffer(void) {
+  event_log_sync();
+  if  (__get_cpu_var(missed_events) > 0)
+       event_log_missed_count(&__get_cpu_var(missed_events));
+}
+
+inline static struct sbuffer* __get_new_cpu_buffer(void) {
+  struct sbuffer* buf = queue_take_try(&empty_buffers);
+  if (NULL == buf) 
+    goto out;
+  __get_cpu_var(cpu_buffers) = buf;
+  init_new_buffer();
+ out:
+  return buf;
+}
+
+inline static struct sbuffer* __get_cpu_buffer(void) {
+  struct sbuffer* buf = __get_cpu_var(cpu_buffers);
+  if (NULL == buf) 
+    buf = __get_new_cpu_buffer();
+  return buf;
+}
+
+static struct sbuffer* __flush_cpu_buffer(void) {
+  struct sbuffer* buf = __get_cpu_var(cpu_buffers);
+  if (NULL != buf)
+    queue_put(&full_buffers, buf);
+  __get_cpu_var(cpu_buffers) = NULL;
+  return __get_cpu_buffer();
+}
+
+/* If not enough space, returns NULL and logs a missed event. */
+void* reserve_event(int len) {
+  struct sbuffer* buf;
+  void* wp;
+
+  /* Get buffer, if available */
+  buf = __get_cpu_buffer();
+ check_buffer:
+  if (!buf) {
+    __get_cpu_var(missed_events)++;
+    return NULL;
+  }
+
+  wp = sbuffer_reserve(buf, len);
+  /* if full, get new buffer */
+  if (!wp) {
+    buf = __flush_cpu_buffer();
+    goto check_buffer;
+  }
+
+  return wp;
+}
+
+static void schedule_compression(void);
+
+void poke_queues(void) {
+  schedule_compression();
+}
+
+void shrink_event(int len) {
+  struct sbuffer* buf;
+  buf = __get_cpu_buffer();
+  if (buf)
+    sbuffer_cancel(buf, len);
+}
+
+/* Returns a reference to the per-cpu timestamp of the last record */
+struct timeval* get_timestamp(void) {
+  return &__get_cpu_var(last_tv);
+}
+
+/*
+ * Must be called with hotplugging disabled
+ * and 'cpu' offline.
+ */
+static void __flush_offline_cpu_buffer(int cpu) {
+  struct sbuffer* buf = per_cpu(cpu_buffers, cpu);
+  printk("eventlogging: flushing offline cpu: %d\n", cpu);
+  if (NULL == buf)
+    return;
+  queue_put(&full_buffers, buf); 
+  per_cpu(cpu_buffers, cpu) = NULL;
+}
+
+/*
+ * Must be called with hotplugging disabled.
+ */
+static void __flush_offline_cpus(void) {
+  int cpu;
+  for_each_cpu_not(cpu, cpu_online_mask) {
+    __flush_offline_cpu_buffer(cpu);
+  }
+}
+
+static void __flush_online_cpu(void* info) {
+  preempt_disable();
+  printk("eventlogging: flushing online cpu: %d\n", smp_processor_id());
+  __flush_cpu_buffer();
+  preempt_enable();
+}
+
+/*
+ * Might sleep, so must be called in sleepable context.
+ */
+void flush_all_cpus(void) {
+  get_online_cpus(); // Disable hotplugging
+  preempt_disable();
+
+  on_each_cpu(__flush_online_cpu, NULL, 1); // Only runs on online cpus
+  __flush_offline_cpus();
+
+  preempt_enable();
+  put_online_cpus(); // Enable hotplugging
+}
+
+static __init int init_alloc_buffers(void) {
+  int i, cpu;
+  int cnt = 0;
+  
+  /* Allocate all buffers */
+  for(i = 0; i < NUM_BUFFERS; ++i) {
+    struct sbuffer* buf;
+
+    buf = (struct sbuffer*) kmalloc(sizeof(struct sbuffer), GFP_ATOMIC);
+    if (0 == buf) {
+      printk("eventlogging: failed to allocate buffer\n");
+      continue;
+    }
+
+    ++cnt;
+    sbuffer_init(buf, BUFFER_ORDER);
+    queue_put(&empty_buffers, buf);
+  }
+  printk("eventlogging: allocated %d buffers\n", cnt);
+
+  /* Set up CPUs to grab new buffer on first event */
+  for_each_cpu(cpu, cpu_possible_mask) {
+    per_cpu(cpu_buffers, cpu) = NULL; 
+    printk("eventlogging: prepare buffer for CPU %d\n", cpu);
+  }
+
+  /* Allocate empty buffer for compression */
+  compress_empty_buffer = queue_take_try(&empty_buffers);
+  if (!compress_empty_buffer)
+    printk(KERN_ERR "eventlogging: failed to allocate empty buffer for compression\n");
+
+  return 0;
+}
+
+/* ============================= Compression ================================ */
+static char lzo_work_mem[LZO1X_1_MEM_COMPRESS];
+
+static int compress_buffer(struct sbuffer* buf) {
+  int err;
+  u32 compressed_len;
+
+  err = mutex_lock_interruptible(&compress_lock);
+  if (err)
+    return err;
+
+  /* Try to get empty buffer, if one is not already available. This
+     should never happen. */
+  if (!compress_empty_buffer && 
+      !(compress_empty_buffer = queue_take_try(&empty_buffers)))
+    goto out;
+
+  sbuffer_clear(compress_empty_buffer);
+
+  /* Reserve four bytes to record data size */
+  compress_empty_buffer->wp += 4;
+
+  compressed_len = compress_empty_buffer->end - compress_empty_buffer->start;
+  err = lzo1x_1_compress(buf->rp, (buf->wp - buf->rp), compress_empty_buffer->wp, &compressed_len, &lzo_work_mem);
+  if (err) {
+    printk(KERN_ERR "eventlogging: error compressing buffer: %d", err);
+    goto out;
+  }
+  compress_empty_buffer->wp += compressed_len;
+  memcpy(compress_empty_buffer->start, &compressed_len, 4);
+
+  sbuffer_swap(compress_empty_buffer, buf);
+  err = 0;
+
+ out:
+  mutex_unlock(&compress_lock);
+  return err;
+}
+
+static void compress_buffer_func(struct work_struct* work) {
+  int ret;
+  struct sbuffer* buf;
+  
+  buf = container_of(work, struct sbuffer, work);
+  ret = compress_buffer(buf);  
+  if (ret)
+    goto err;
+
+  queue_put(&compressed_buffers, buf);
+  queue_poke(&compressed_buffers);
+  return;
+
+ err:
+  printk("eventlogging: failed to compress buffer: %d", ret);
+}
+
+static void schedule_compression(void) {
+  struct sbuffer* buf;
+  while( (buf = queue_take_try(&full_buffers)) ) {
+    INIT_WORK(&buf->work, compress_buffer_func);
+    schedule_work(&buf->work);
+  }
+}
+
+/* =========================== Proc FS Methods ============================== */
+static int event_logging_read_pfs_restart(void) {
+  int err;
+
+  err = mutex_lock_interruptible(&pfs_read_lock);
+  if (err)
+    goto mutex_err;
+
+  /* If read is incomplete, restart read of this buffer */
+  if (NULL != pfs_read_buffer && !sbuffer_empty(pfs_read_buffer)) {
+    printk(KERN_INFO "eventlogging: restarting read of buffer");
+    sbuffer_restart_read(pfs_read_buffer);
+  }
+
+  mutex_unlock(&pfs_read_lock);
+  return 0;
+
+ mutex_err:
+  return err;
+}
+
+/**
+ * Flush the current buffers and then remove all pending,
+ * unread compressed buffers.
+ */
+static int event_logging_read_pfs_clear(void) {
+  int err;
+  struct sbuffer* buf;
+  int cnt = 0;
+
+  err = mutex_lock_interruptible(&pfs_read_lock);
+  if (err)
+    goto mutex_err;
+
+  /* Flush all cpus */
+  flush_all_cpus();
+
+  /* Remove all from full buffers queue. */
+  /* TODO: This removal races with the poke_queues() method, so the
+   * buffer might still sneak into a compression task and then onto
+   * the compressed buffers queue.  It's unlikely so I haven't fixed
+   * that yet.
+   */
+  while( (buf = queue_take_try(&full_buffers)) ) {
+    ++cnt;
+    sbuffer_clear(buf);
+    queue_put(&empty_buffers, buf);
+  }
+
+  /* Return buffer currently being read to empty queue*/ 
+  if (NULL != pfs_read_buffer) {
+    ++cnt;
+    sbuffer_clear(pfs_read_buffer);
+    queue_put(&empty_buffers, pfs_read_buffer);
+    pfs_read_buffer = NULL;
+  }
+
+  /* Remove all from compressed buffers queue */
+  while ( (buf = queue_take_try(&compressed_buffers)) ) {
+    ++cnt;
+    sbuffer_clear(buf);
+    queue_put(&empty_buffers, buf);
+  }
+  printk(KERN_INFO "eventlogging: cleared %d unread buffers", cnt);
+
+  mutex_unlock(&pfs_read_lock);
+  return 0;
+
+ mutex_err:
+  return err;
+}
+
+static int event_logging_read_pfs(char* page, char** start, off_t off, int count, int* eof, void* data) {
+  int err, len;
+
+  len = 0;
+  *start = page;
+  *eof = 1;
+
+  err = mutex_lock_interruptible(&pfs_read_lock);
+  if (err)
+    goto err;
+
+  while (len == 0) {
+    /* Return now-empty buffer to empty queue */
+    if (NULL != pfs_read_buffer && sbuffer_empty(pfs_read_buffer)) {
+      sbuffer_clear(pfs_read_buffer);
+      queue_put(&empty_buffers, pfs_read_buffer);
+      pfs_read_buffer = NULL;
+    }
+
+    /* Get a new buffer from the full queue */
+    if (NULL == pfs_read_buffer) {
+      pfs_read_buffer = queue_take_interruptible(&compressed_buffers);
+      if (IS_ERR(pfs_read_buffer)) {
+	err = PTR_ERR(pfs_read_buffer);
+	pfs_read_buffer = NULL;
+	goto err;
+      }
+    }
+    
+    /* Read from the buffer */
+    len += sbuffer_read(pfs_read_buffer, page, count);
+  }
+  
+  mutex_unlock(&pfs_read_lock);
+  return len;
+
+  err:
+    mutex_unlock(&pfs_read_lock);
+    return err;
+}
+
+static int event_logging_write_pfs(struct file* file, const char* buffer, unsigned long count, void *data) {
+  int err;
+  char command[PFS_COMMAND_LEN+1];
+
+  if (!count)
+    return 0;
+
+  memset(command, 0, sizeof(command));
+  if (count > PFS_COMMAND_LEN)
+    count = PFS_COMMAND_LEN;
+
+  if ( copy_from_user(command, buffer, count) )
+    return -EFAULT;
+
+  /* Process restart command */
+  if ( 0 == strcmp(command, PFS_RESTART) ) {
+    err = event_logging_read_pfs_restart();
+    if (err)
+      goto err;
+  }
+  /* Process clear command */
+  else if (0 == strcmp(command, PFS_CLEAR) ) {
+    err = event_logging_read_pfs_clear();
+    if (err)
+      goto err;
+  }
+  /* Process default command */
+  else {
+    flush_all_cpus();
+  }
+
+  return count;
+
+ err:
+  return err;
+}
+
+static __init int event_logging_create_pfs(void) {
+  el_pfs_entry = create_proc_entry(PFS_NAME, PFS_PERMS, NULL);
+  if (!el_pfs_entry)
+    goto err;
+  
+  el_pfs_entry->uid = 0;
+  el_pfs_entry->gid = 0;
+  el_pfs_entry->read_proc = event_logging_read_pfs;
+  el_pfs_entry->write_proc = event_logging_write_pfs;
+  return 0;
+
+ err:
+  return -EINVAL;
+}
+
+/* ========================= Initialization Config ========================== */
+
+early_initcall(init_alloc_buffers);
+early_initcall(init_idle_notifier);
+early_initcall(init_hotcpu_notifier);
+fs_initcall(init_cpufreq_notifier);
+fs_initcall(event_logging_create_pfs);
+
+
diff --git a/kernel/eventlogging/logging.h b/kernel/eventlogging/logging.h
new file mode 100644
index 0000000..fbeb850
--- /dev/null
+++ b/kernel/eventlogging/logging.h
@@ -0,0 +1,9 @@
+#ifndef EVENT_LOGGING_H
+#define EVENT_LOGGING_H
+
+#include "buffer.h"
+
+void flush_all_cpus(void);
+void log_event(void* data, int len);
+
+#endif
diff --git a/kernel/eventlogging/queue.h b/kernel/eventlogging/queue.h
new file mode 100644
index 0000000..e199c42
--- /dev/null
+++ b/kernel/eventlogging/queue.h
@@ -0,0 +1,107 @@
+#ifndef EVENT_LOGGING_QUEUE_H
+#define EVENT_LOGGING_QUEUE_H
+
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/wait.h>
+#include <linux/err.h>
+
+#include "buffer.h"
+
+struct queue {
+  struct list_head list;
+  spinlock_t lock;
+  unsigned long flags;
+  wait_queue_head_t wait;
+};
+
+#define DEFINE_QUEUE(name) struct queue name = {	\
+    .lock = __SPIN_LOCK_UNLOCKED(name.lock),		\
+    .list = LIST_HEAD_INIT(name.list),			\
+    .wait = __WAIT_QUEUE_HEAD_INITIALIZER(name.wait)	\
+  }						 
+
+static inline void queue_lock(struct queue* queue) {
+  spin_lock_irqsave(&queue->lock, queue->flags);
+}
+
+static inline void queue_unlock(struct queue* queue) {
+  spin_unlock_irqrestore(&queue->lock, queue->flags);
+}
+
+static inline int queue_empty(struct queue* queue) {
+  int ret;
+  queue_lock(queue);
+  ret = list_empty(&queue->list);
+  queue_unlock(queue);
+  return ret;
+}
+
+static inline void queue_put(struct queue* queue, struct sbuffer *buf) {
+  queue_lock(queue);
+  list_add_tail(&buf->list, &queue->list);
+  queue_unlock(queue);
+}
+
+/* Wakes up any blocked peekers or takers if the queue is not empty */
+static inline void queue_poke(struct queue* queue) {
+  queue_lock(queue);
+  if (unlikely(!list_empty(&queue->list)))
+    wake_up_interruptible(&queue->wait);
+  queue_unlock(queue);
+}
+
+static inline struct sbuffer* __queue_peek_try(struct queue* queue) {
+  if (!list_empty(&queue->list))
+    return list_entry(queue->list.next, struct sbuffer, list);
+  else
+    return NULL;
+}
+
+static inline struct sbuffer* queue_take_try(struct queue* queue) {
+  struct sbuffer* buf = NULL;
+  queue_lock(queue);
+  buf = __queue_peek_try(queue);
+  if (buf != NULL)
+    list_del_init(&buf->list);
+  queue_unlock(queue);
+  return buf;
+}
+
+static inline struct sbuffer* queue_take_interruptible(struct queue* queue) {
+  struct sbuffer* ret = NULL;
+  int err = 0;
+  do {
+    ret = queue_take_try(queue);
+  } while ( NULL == ret &&
+	    0 == (err = wait_event_interruptible(queue->wait, !queue_empty(queue))) );
+  
+  if (err)
+    return ERR_PTR(err);
+  else
+    return ret;
+}
+
+static inline struct sbuffer* queue_peek_try(struct queue* queue) {
+  struct sbuffer* buf = NULL;
+  queue_lock(queue);
+  buf = __queue_peek_try(queue);
+  queue_unlock(queue);
+  return buf;
+}
+
+static inline struct sbuffer* queue_peek_interruptible(struct queue* queue) {
+  struct sbuffer* ret = NULL;
+  int err = 0;
+  do {
+    ret = queue_peek_try(queue);
+  } while ( NULL == ret && 
+	    0 == (err = wait_event_interruptible(queue->wait, !queue_empty(queue))) );
+
+  if (err)
+    return ERR_PTR(err);
+  else
+    return ret;
+}
+
+#endif
diff --git a/kernel/exit.c b/kernel/exit.c
index 303bed2..4adf3ff 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -52,6 +52,8 @@
 #include <linux/hw_breakpoint.h>
 #include <linux/oom.h>
 
+#include <eventlogging/events.h>
+
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
 #include <asm/pgtable.h>
@@ -905,6 +907,7 @@ NORET_TYPE void do_exit(long code)
 	int group_dead;
 
 	profile_task_exit(tsk);
+	event_log_exit();
 
 	WARN_ON(atomic_read(&tsk->fs_excl));
 	WARN_ON(blk_needs_flush_plug(tsk));
diff --git a/kernel/fork.c b/kernel/fork.c
index 06909a9..8f358a3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -69,6 +69,8 @@
 #include <linux/khugepaged.h>
 #include <linux/signalfd.h>
 
+#include <eventlogging/events.h>
+
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -1269,6 +1271,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (clone_flags & CLONE_THREAD)
 		p->tgid = current->tgid;
 
+	event_log_fork(p->pid, p->tgid);
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
 	/*
 	 * Clear TID on mm_release()?
diff --git a/kernel/futex.c b/kernel/futex.c
index b2d51a7..623ef95 100644
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@ -63,6 +63,8 @@
 
 #include <asm/futex.h>
 
+#include <eventlogging/events.h>
+
 #include "rtmutex_common.h"
 
 int __read_mostly futex_cmpxchg_enabled;
@@ -836,7 +838,7 @@ static void __unqueue_futex(struct futex_q *q)
  * The hash bucket lock must be held when this is called.
  * Afterwards, the futex_q must not be accessed.
  */
-static void wake_futex(struct futex_q *q)
+static void wake_futex(u32 __user *uaddr, struct futex_q *q)
 {
 	struct task_struct *p = q->task;
 
@@ -859,6 +861,7 @@ static void wake_futex(struct futex_q *q)
 	smp_wmb();
 	q->lock_ptr = NULL;
 
+	event_log_futex_notify(uaddr, p->pid);
 	wake_up_state(p, TASK_NORMAL);
 	put_task_struct(p);
 }
@@ -1001,7 +1004,7 @@ futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
 			if (!(this->bitset & bitset))
 				continue;
 
-			wake_futex(this);
+			wake_futex(uaddr, this);
 			if (++ret >= nr_wake)
 				break;
 		}
@@ -1075,7 +1078,7 @@ retry_private:
 
 	plist_for_each_entry_safe(this, next, head, list) {
 		if (match_futex (&this->key, &key1)) {
-			wake_futex(this);
+ 			wake_futex(uaddr1, this);
 			if (++ret >= nr_wake)
 				break;
 		}
@@ -1087,7 +1090,7 @@ retry_private:
 		op_ret = 0;
 		plist_for_each_entry_safe(this, next, head, list) {
 			if (match_futex (&this->key, &key2)) {
-				wake_futex(this);
+				wake_futex(uaddr2, this);
 				if (++op_ret >= nr_wake2)
 					break;
 			}
@@ -1397,7 +1400,7 @@ retry_private:
 		 * woken by futex_unlock_pi().
 		 */
 		if (++task_count <= nr_wake && !requeue_pi) {
-			wake_futex(this);
+			wake_futex(uaddr1, this);
 			continue;
 		}
 
@@ -1756,7 +1759,7 @@ out:
  * @q:		the futex_q to queue up on
  * @timeout:	the prepared hrtimer_sleeper, or null for no timeout
  */
-static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,
+static void futex_wait_queue_me(u32 __user *uaddr, struct futex_hash_bucket *hb, struct futex_q *q,
 				struct hrtimer_sleeper *timeout)
 {
 	/*
@@ -1785,8 +1788,11 @@ static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,
 		 * flagged for rescheduling. Only call schedule if there
 		 * is no timeout, or if it has yet to expire.
 		 */
-		if (!timeout || timeout->task)
+		if (!timeout || timeout->task) {
+			event_log_futex_wait(uaddr);
 			schedule();
+			event_log_futex_wake(uaddr);
+		}
 	}
 	__set_current_state(TASK_RUNNING);
 }
@@ -1901,7 +1907,7 @@ retry:
 		goto out;
 
 	/* queue_me and wait for wakeup, timeout, or a signal. */
-	futex_wait_queue_me(hb, &q, to);
+	futex_wait_queue_me(uaddr, hb, &q, to);
 
 	/* If we were woken (and unqueued), we succeeded, whatever. */
 	ret = 0;
@@ -2309,7 +2315,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 		goto out_key2;
 
 	/* Queue the futex_q, drop the hb lock, wait for wakeup. */
-	futex_wait_queue_me(hb, &q, to);
+	futex_wait_queue_me(uaddr, hb, &q, to);
 
 	spin_lock(&hb->lock);
 	ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4ba7ccc..62dd466 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -18,6 +18,8 @@
 #include <linux/freezer.h>
 #include <trace/events/sched.h>
 
+#include <eventlogging/events.h>
+
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
 struct task_struct *kthreadd_task;
@@ -174,6 +176,7 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 		vsnprintf(create.result->comm, sizeof(create.result->comm),
 			  namefmt, args);
 		va_end(args);
+		event_log_thread_name(create.result);
 		/*
 		 * root may have changed our (kthreadd's) priority or CPU mask.
 		 * The kernel thread should not inherit these properties.
diff --git a/kernel/mutex.c b/kernel/mutex.c
index d607ed5..8804b06 100644
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@ -24,6 +24,8 @@
 #include <linux/interrupt.h>
 #include <linux/debug_locks.h>
 
+#include <eventlogging/events.h>
+
 /*
  * In the DEBUG case we are using the "NULL fastpath" for mutexes,
  * which forces all calls into the slowpath:
@@ -241,13 +243,16 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 		/* didn't get the lock, go to sleep: */
 		spin_unlock_mutex(&lock->wait_lock, flags);
 		preempt_enable_no_resched();
+		event_log_mutex_wait(lock);
 		schedule();
+		event_log_mutex_wake(lock);
 		preempt_disable();
 		spin_lock_mutex(&lock->wait_lock, flags);
 	}
 
 done:
 	lock_acquired(&lock->dep_map, ip);
+	event_log_mutex_lock(lock);
 	/* got the lock - rejoice! */
 	mutex_remove_waiter(lock, &waiter, current_thread_info());
 	mutex_set_owner(lock);
@@ -330,7 +335,7 @@ __mutex_unlock_common_slowpath(atomic_t *lock_count, int nested)
 					   struct mutex_waiter, list);
 
 		debug_mutex_wake_waiter(lock, waiter);
-
+		event_log_mutex_notify(lock, waiter->task->pid);
 		wake_up_process(waiter->task);
 	}
 
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 61e6347..245d630 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -27,6 +27,8 @@
 
 #include "power.h"
 
+#include <eventlogging/events.h>
+
 const char *const pm_states[PM_SUSPEND_MAX] = {
 #ifdef CONFIG_EARLYSUSPEND
 	[PM_SUSPEND_ON]		= "on",
@@ -166,6 +168,7 @@ static int suspend_enter(suspend_state_t state)
 	arch_suspend_disable_irqs();
 	BUG_ON(!irqs_disabled());
 
+	event_log_suspend();
 	error = syscore_suspend();
 	if (!error) {
 		if (!(suspend_test(TEST_CORE) || pm_wakeup_pending())) {
@@ -173,6 +176,7 @@ static int suspend_enter(suspend_state_t state)
 			events_check_enabled = false;
 		}
 		syscore_resume();
+		event_log_resume();
 	}
 
 	arch_suspend_enable_irqs();
@@ -276,6 +280,8 @@ int enter_state(suspend_state_t state)
 	if (!mutex_trylock(&pm_mutex))
 		return -EBUSY;
 
+	event_log_suspend_start();
+
 	printk(KERN_INFO "PM: Syncing filesystems ... ");
 	sys_sync();
 	printk("done.\n");
@@ -298,6 +304,7 @@ int enter_state(suspend_state_t state)
 	suspend_finish();
  Unlock:
 	mutex_unlock(&pm_mutex);
+	event_log_resume_finish();
 	return error;
 }
 
@@ -310,8 +317,8 @@ int enter_state(suspend_state_t state)
  */
 int pm_suspend(suspend_state_t state)
 {
-	if (state > PM_SUSPEND_ON && state < PM_SUSPEND_MAX)
-		return enter_state(state);
-	return -EINVAL;
+  if (state > PM_SUSPEND_ON && state < PM_SUSPEND_MAX)
+	return enter_state(state);
+  return -EINVAL;
 }
 EXPORT_SYMBOL(pm_suspend);
diff --git a/kernel/power/wakelock.c b/kernel/power/wakelock.c
index 81e1b7c..3e077bc 100644
--- a/kernel/power/wakelock.c
+++ b/kernel/power/wakelock.c
@@ -24,6 +24,8 @@
 #endif
 #include "power.h"
 
+#include <eventlogging/events.h>
+
 enum {
 	DEBUG_EXIT_SUSPEND = 1U << 0,
 	DEBUG_WAKEUP = 1U << 1,
@@ -489,12 +491,14 @@ static void wake_lock_internal(
 
 void wake_lock(struct wake_lock *lock)
 {
+	event_log_wake_lock(lock, 0);
 	wake_lock_internal(lock, 0, 0);
 }
 EXPORT_SYMBOL(wake_lock);
 
 void wake_lock_timeout(struct wake_lock *lock, long timeout)
 {
+	event_log_wake_lock(lock, timeout);
 	wake_lock_internal(lock, timeout, 1);
 }
 EXPORT_SYMBOL(wake_lock_timeout);
@@ -505,6 +509,7 @@ void wake_unlock(struct wake_lock *lock)
 	unsigned long irqflags;
 	spin_lock_irqsave(&list_lock, irqflags);
 	type = lock->flags & WAKE_LOCK_TYPE_MASK;
+	event_log_wake_unlock(lock);
 #ifdef CONFIG_WAKELOCK_STAT
 	wake_unlock_stat_locked(lock, 0);
 #endif
diff --git a/kernel/sched.c b/kernel/sched.c
index d488880..de7ea46 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -73,6 +73,8 @@
 #include <linux/slab.h>
 #include <linux/cpuacct.h>
 
+#include <eventlogging/events.h>
+
 #include <asm/tlb.h>
 #include <asm/irq_regs.h>
 #include <asm/mutex.h>
@@ -3144,6 +3146,8 @@ context_switch(struct rq *rq, struct task_struct *prev,
 {
 	struct mm_struct *mm, *oldmm;
 
+	event_log_context_switch(next->pid, prev->state);
+
 	prepare_task_switch(rq, prev, next);
 
 	mm = next->mm;
@@ -4314,9 +4318,14 @@ static inline void sched_submit_work(struct task_struct *tsk)
 asmlinkage void __sched schedule(void)
 {
 	struct task_struct *tsk = current;
-
+	
 	sched_submit_work(tsk);
 	__schedule();
+#ifdef CONFIG_EVENT_LOGGING
+	/* Cannot wake up blocked peekers and takers when logging the
+	   event in context_switch(), so manually poke them here. */
+	poke_queues();
+#endif
 }
 EXPORT_SYMBOL(schedule);
 
@@ -4456,7 +4465,18 @@ static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
 
 	list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
 		unsigned flags = curr->flags;
-
+		/* the waitqueuentry can be initialized either with
+                 * the default wake function and the private void*
+                 * pointing to the task struct or with a custom wake
+                 * function and the private void* set to NULL.  We are
+                 * only interested in the default wake function (which
+                 * actually wakes a process), so we only log an event
+                 * with when the private void* is non-null.  This also
+                 * makes the casting to (struct task_struct*) safe,
+                 * although future kernels could break this.
+		 */
+		if (curr->private)
+			event_log_waitqueue_notify(q, ((struct task_struct*)curr->private)->pid);
 		if (curr->func(curr, mode, wake_flags, key) &&
 				(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
 			break;
@@ -5531,6 +5551,7 @@ SYSCALL_DEFINE0(sched_yield)
 	do_raw_spin_unlock(&rq->lock);
 	preempt_enable_no_resched();
 
+	event_log_yield();
 	schedule();
 
 	return 0;
@@ -5667,8 +5688,10 @@ out:
 	double_rq_unlock(rq, p_rq);
 	local_irq_restore(flags);
 
-	if (yielded)
+	if (yielded) {
+		event_log_yield();
 		schedule();
+	}
 
 	return yielded;
 }
@@ -5686,7 +5709,9 @@ void __sched io_schedule(void)
 	atomic_inc(&rq->nr_iowait);
 	blk_flush_plug(current);
 	current->in_iowait = 1;
+	event_log_io_block();
 	schedule();
+	event_log_io_resume();
 	current->in_iowait = 0;
 	atomic_dec(&rq->nr_iowait);
 	delayacct_blkio_end();
@@ -5702,7 +5727,9 @@ long __sched io_schedule_timeout(long timeout)
 	atomic_inc(&rq->nr_iowait);
 	blk_flush_plug(current);
 	current->in_iowait = 1;
+	event_log_io_block();
 	ret = schedule_timeout(timeout);
+	event_log_io_resume();
 	current->in_iowait = 0;
 	atomic_dec(&rq->nr_iowait);
 	delayacct_blkio_end();
diff --git a/kernel/sched_fair.c b/kernel/sched_fair.c
index c768588..7a289c8 100644
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -1100,6 +1100,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 	ideal_runtime = sched_slice(cfs_rq, curr);
 	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
 	if (delta_exec > ideal_runtime) {
+		event_log_preempt_tick();
 		resched_task(rq_of(cfs_rq)->curr);
 		/*
 		 * The current task ran long enough, ensure it doesn't get
@@ -1127,8 +1128,10 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 		if (delta < 0)
 			return;
 
-		if (delta > ideal_runtime)
+		if (delta > ideal_runtime) {
+			event_log_preempt_tick();
 			resched_task(rq_of(cfs_rq)->curr);
+		}
 	}
 }
 
@@ -1937,6 +1940,7 @@ static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_
 	return;
 
 preempt:
+	event_log_preempt_wakeup();
 	resched_task(curr);
 	/*
 	 * Only set the backward buddy when the current task is still
diff --git a/kernel/semaphore.c b/kernel/semaphore.c
index 94a62c0..884b238 100644
--- a/kernel/semaphore.c
+++ b/kernel/semaphore.c
@@ -33,6 +33,8 @@
 #include <linux/spinlock.h>
 #include <linux/ftrace.h>
 
+#include <eventlogging/events.h>
+
 static noinline void __down(struct semaphore *sem);
 static noinline int __down_interruptible(struct semaphore *sem);
 static noinline int __down_killable(struct semaphore *sem);
@@ -55,10 +57,13 @@ void down(struct semaphore *sem)
 	unsigned long flags;
 
 	spin_lock_irqsave(&sem->lock, flags);
-	if (likely(sem->count > 0))
+	if (likely(sem->count > 0)) {
+		event_log_sem_lock(sem);
 		sem->count--;
-	else
+	}
+	else {
 		__down(sem);
+	}
 	spin_unlock_irqrestore(&sem->lock, flags);
 }
 EXPORT_SYMBOL(down);
@@ -78,10 +83,13 @@ int down_interruptible(struct semaphore *sem)
 	int result = 0;
 
 	spin_lock_irqsave(&sem->lock, flags);
-	if (likely(sem->count > 0))
+	if (likely(sem->count > 0)) {
+		event_log_sem_lock(sem);
 		sem->count--;
-	else
+	}
+	else {
 		result = __down_interruptible(sem);
+	}
 	spin_unlock_irqrestore(&sem->lock, flags);
 
 	return result;
@@ -104,10 +112,13 @@ int down_killable(struct semaphore *sem)
 	int result = 0;
 
 	spin_lock_irqsave(&sem->lock, flags);
-	if (likely(sem->count > 0))
+	if (likely(sem->count > 0)) {
+		event_log_sem_lock(sem);
 		sem->count--;
-	else
+	}
+	else {
 		result = __down_killable(sem);
+	}
 	spin_unlock_irqrestore(&sem->lock, flags);
 
 	return result;
@@ -134,8 +145,10 @@ int down_trylock(struct semaphore *sem)
 
 	spin_lock_irqsave(&sem->lock, flags);
 	count = sem->count - 1;
-	if (likely(count >= 0))
+	if (likely(count >= 0)) {
+		event_log_sem_lock(sem);
 		sem->count = count;
+	}
 	spin_unlock_irqrestore(&sem->lock, flags);
 
 	return (count < 0);
@@ -158,10 +171,13 @@ int down_timeout(struct semaphore *sem, long jiffies)
 	int result = 0;
 
 	spin_lock_irqsave(&sem->lock, flags);
-	if (likely(sem->count > 0))
+	if (likely(sem->count > 0)) {
+		event_log_sem_lock(sem);
 		sem->count--;
-	else
+	}
+	else {
 		result = __down_timeout(sem, jiffies);
+	}
 	spin_unlock_irqrestore(&sem->lock, flags);
 
 	return result;
@@ -211,6 +227,7 @@ static inline int __sched __down_common(struct semaphore *sem, long state,
 	waiter.task = task;
 	waiter.up = 0;
 
+	event_log_sem_wait(sem);
 	for (;;) {
 		if (signal_pending_state(state, task))
 			goto interrupted;
@@ -220,15 +237,19 @@ static inline int __sched __down_common(struct semaphore *sem, long state,
 		spin_unlock_irq(&sem->lock);
 		timeout = schedule_timeout(timeout);
 		spin_lock_irq(&sem->lock);
-		if (waiter.up)
+		if (waiter.up) {
+			event_log_sem_wake(sem);
 			return 0;
+		}
 	}
 
  timed_out:
+	event_log_sem_wake(sem);
 	list_del(&waiter.list);
 	return -ETIME;
 
  interrupted:
+	event_log_sem_wake(sem);
 	list_del(&waiter.list);
 	return -EINTR;
 }
@@ -259,5 +280,6 @@ static noinline void __sched __up(struct semaphore *sem)
 						struct semaphore_waiter, list);
 	list_del(&waiter->list);
 	waiter->up = 1;
+	event_log_sem_notify(sem, waiter->task->pid);
 	wake_up_process(waiter->task);
 }
diff --git a/kernel/wait.c b/kernel/wait.c
index f45ea8d..4abef37 100644
--- a/kernel/wait.c
+++ b/kernel/wait.c
@@ -68,11 +68,12 @@ void
 prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)
 {
 	unsigned long flags;
-
 	wait->flags &= ~WQ_FLAG_EXCLUSIVE;
 	spin_lock_irqsave(&q->lock, flags);
-	if (list_empty(&wait->task_list))
+	if (list_empty(&wait->task_list)) {
 		__add_wait_queue(q, wait);
+		event_log_waitqueue_wait(q);
+	}
 	set_current_state(state);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
@@ -85,8 +86,10 @@ prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state)
 
 	wait->flags |= WQ_FLAG_EXCLUSIVE;
 	spin_lock_irqsave(&q->lock, flags);
-	if (list_empty(&wait->task_list))
+	if (list_empty(&wait->task_list)) {
 		__add_wait_queue_tail(q, wait);
+		event_log_waitqueue_wait(q);
+	}
 	set_current_state(state);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
@@ -105,6 +108,7 @@ void finish_wait(wait_queue_head_t *q, wait_queue_t *wait)
 {
 	unsigned long flags;
 
+	event_log_waitqueue_wake(q);
 	__set_current_state(TASK_RUNNING);
 	/*
 	 * We can check for list emptiness outside the lock
@@ -152,8 +156,10 @@ void abort_exclusive_wait(wait_queue_head_t *q, wait_queue_t *wait,
 
 	__set_current_state(TASK_RUNNING);
 	spin_lock_irqsave(&q->lock, flags);
-	if (!list_empty(&wait->task_list))
+	if (!list_empty(&wait->task_list)) {
 		list_del_init(&wait->task_list);
+		event_log_waitqueue_wake(q);
+	}
 	else if (waitqueue_active(q))
 		__wake_up_locked_key(q, mode, key);
 	spin_unlock_irqrestore(&q->lock, flags);
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index c6b006a..c38c2c4 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -1109,6 +1109,7 @@ config SYSCTL_SYSCALL_CHECK
 
 source mm/Kconfig.debug
 source kernel/trace/Kconfig
+source kernel/eventlogging/Kconfig
 
 config PROVIDE_OHCI1394_DMA_INIT
 	bool "Remote debugging over FireWire early on boot"
diff --git a/net/core/datagram.c b/net/core/datagram.c
index 18ac112..526e594 100644
--- a/net/core/datagram.c
+++ b/net/core/datagram.c
@@ -58,6 +58,8 @@
 #include <net/tcp_states.h>
 #include <trace/events/skb.h>
 
+#include <eventlogging/events.h>
+
 /*
  *	Is a socket 'connection oriented' ?
  */
@@ -113,7 +115,9 @@ static int wait_for_packet(struct sock *sk, int *err, long *timeo_p)
 		goto interrupted;
 
 	error = 0;
+	event_log_datagram_block();
 	*timeo_p = schedule_timeout(*timeo_p);
+	event_log_datagram_resume();
 out:
 	finish_wait(sk_sleep(sk), &wait);
 	return error;
diff --git a/net/core/sock.c b/net/core/sock.c
index aebb419..aec9fb3 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -132,6 +132,8 @@
 #include <net/tcp.h>
 #endif
 
+#include <eventlogging/events.h>
+
 /*
  * Each address family might have different locking rules, so we have
  * one slock key per address family:
@@ -1657,7 +1659,9 @@ int sk_wait_data(struct sock *sk, long *timeo)
 
 	prepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);
 	set_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);
+	event_log_sock_block();
 	rc = sk_wait_event(sk, timeo, !skb_queue_empty(&sk->sk_receive_queue));
+	event_log_sock_resume();
 	clear_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);
 	finish_wait(sk_sleep(sk), &wait);
 	return rc;
diff --git a/net/core/stream.c b/net/core/stream.c
index f5df85d..d39f3c4 100644
--- a/net/core/stream.c
+++ b/net/core/stream.c
@@ -19,6 +19,8 @@
 #include <linux/wait.h>
 #include <net/sock.h>
 
+#include <eventlogging/events.h>
+
 /**
  * sk_stream_write_space - stream socket write_space callback.
  * @sk: socket
@@ -71,10 +73,12 @@ int sk_stream_wait_connect(struct sock *sk, long *timeo_p)
 
 		prepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);
 		sk->sk_write_pending++;
+		event_log_stream_block();
 		done = sk_wait_event(sk, timeo_p,
 				     !sk->sk_err &&
 				     !((1 << sk->sk_state) &
 				       ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)));
+		event_log_stream_resume();
 		finish_wait(sk_sleep(sk), &wait);
 		sk->sk_write_pending--;
 	} while (!done);
diff --git a/scripts/kconfig/.gitignore b/scripts/kconfig/.gitignore
index 624f650..7d83afb 100644
--- a/scripts/kconfig/.gitignore
+++ b/scripts/kconfig/.gitignore
@@ -6,6 +6,7 @@ lex.*.c
 *.tab.c
 *.tab.h
 zconf.hash.c
+zconf.lex.c
 *.moc
 lkc_defs.h
 gconf.glade.h
